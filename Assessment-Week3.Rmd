---
title: "<img src='figure/coursera.jpg' width='37'> <img src='figure/ucsc.png' width='240'>"
subtitle: "<span style='color:white; background-color:#4E79A7;'>Bayesian Statistics: Mixture Models</span> (Assessment Week3 with Codes)"
author: "[¬ÆŒ≥œÉ, Lian Hu](https://englianhu.github.io/) <img src='figure/quantitative trader 1.jpg' width='12'> <img src='figure/ENG.jpg' width='24'> ¬Æ"
date: "`r lubridate::today('Asia/Tokyo')`"
output:
  html_document: 
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    css: CSSBackgrounds.css
---

<br>
<span style='color:green'>**Theme Song**</span>
<br>

<audio src="music/California-Dreaming-Chorus.mp3" controls></audio>
<br>

------

## SCSS Setup

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
.table-hover > tbody > tr:hover { 
  background-color: #8D918D;
}
</style>

```{r class.source = 'bg-success', class.output = 'bg-primary', message = FALSE, warning = FALSE}
# install.packages("remotes")
require('BBmisc')
#remotes::install_github("rstudio/sass")
lib('sass')
```

```{scss class.source = 'bg-success', class.output = 'bg-primary'}
/* https://stackoverflow.com/a/66029010/3806250 */
h1 { color: #002C54; }
h2 { color: #2F496E; }
h3 { color: #375E97; }

/* ----------------------------------------------------------------- */
/* https://gist.github.com/himynameisdave/c7a7ed14500d29e58149#file-broken-gradient-animation-less */
.hover-animate-background1 {
  /* color: #FFD64D; */
  background: linear-gradient(155deg, #EDAE01 0%, #FFEB94 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #EDAE01 20%, #FFEB94 80%);
    }
  }

/* //  For brevity, vendor prefixes have been removed. */
/* //  This does not work as expected; instead of a smooth transition */
/* //  what you get is a hard swap from one gradient to the next */
.hover-animate-background2 {
  color: #FFD64D;
  background: linear-gradient(155deg, #002C54 0%, #4CB5F5 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #002C54 20%, #4CB5F5 80%);
    }
  }

.hover-animate-background3 {
  color: #FFD64D;
  background: linear-gradient(155deg, #A10115 0%, #FF3C5C 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #A10115 20%, #FF3C5C 80%);
    }
  }
```

```{r global_options, class.source = 'hover-animate-background1', class.output = 'hover-animate-background2'}
## https://stackoverflow.com/a/36846793/3806250
options(width = 999)
knitr::opts_chunk$set(class.source = 'hover-animate-background1', class.output = 'hover-animate-background2', class.error = 'hover-animate-background3')
```

<br><br>

# ÂèóË¨õÁîü„Å´„Çà„Çã„ÉÜ„Çπ„ÉàÔºöMarkov chain Monte Carlo algorithms for Mixture Models

‰ªñ„ÅÆÂèóË¨õÁîü„ÅÆË™≤È°å„Çí„É¨„Éì„É•„Éº„Åô„Çã

Ë™≤È°å„ÅÆÊèêÂá∫„ÄÅ„ÅäÁñ≤„Çå„Åï„Åæ„Åß„Åó„ÅüÔºÅ„Åì„Çå„Åß„ÄÅ‰ªñ„ÅÆÂèóË¨õÁîü„Åå„É¨„Éì„É•„Éº„Åß„Åç„Åæ„Åô„ÄÇÊàêÁ∏æ„ÇíÂèó„ÅëÂèñ„Çã„Å´„ÅØ„ÄÅ‰ªñ„ÅÆÂèóË¨õÁîü„ÅÆË™≤È°å„ÇÇ„ÅÑ„Åè„Å§„Åã„É¨„Éì„É•„Éº„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ ÊàêÁ∏æ„ÅØ`5Êúà27Êó• 15:59 JST`„Åæ„Åß„Å´Âèó„ÅëÂèñ„Çå„Çã„Åß„Åó„Çá„ÅÜ„ÄÇ

<br><br>

## Ë™¨Êòé

Data on the lifetime (in years) of fuses produced by the ACME Corporation is available in the file `fuses.csv`:

In order to characterize the distribution of the lifetimes, it seems reasonable to fit to the data a two-component mixture of the form:

$$
\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}
$$
The first component, which corresponds to an exponential distribution with rate $\lambda$, is used to model low-quality components with a very short lifetime. The second component, which corresponds to a [log-Gaussian distribution](https://en.wikipedia.org/wiki/Log-normal_distribution), is used to model normal, properly-functioning components.

You are asked to modify the implementation of the MCMC algorithm contained in the Reading "Sample code for MCMC example 1" so that you can fit this two-component mixture distributions instead. You then should run your algorithm for 10,000 iterations after a burn-in period of 1,000 iterations and report your estimates of the posterior means, rounded to two decimal places. Assume the following priors: $\omega‚àºUni[0,1], \lambda‚àºExp(1), \mu‚àºNormal(0,1) $ and $\tau^2‚àºIGam(2,1)$.

<br><br>

### Review criteria

The code you generate should follow the same structure as "Sample code for MCMC example 1". In particular, focus on a Gibss sampler that alternates between the full conditionals for $\omega$, $\lambda$, $\mu$, $\tau^2$ and the latent component indicators $c_1, \ldots,c_n$. Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect the fact that (1) parameters have been initialized in a reasonable way, (2) each of the two full conditional distributions associated with the sampler are correct, and (2) the numerical values that you obtain are correct.  To simplify the peer-review process, assume that component 1 corresponds to the exponential distribution, while component 2 corresponds to the log-Gaussian distribution.

<br><br>

## Ëá™ÂàÜ„ÅÆÊèêÂá∫Áâ©

## „Éî„Ç¢„É¨„Éì„É•„Éº„Éá„Ç£„Çπ„Ç´„ÉÉ„Ç∑„Éß„É≥

### Setup

```{r setup, warning = FALSE, message = FALSE}
if(!suppressPackageStartupMessages(require('BBmisc'))) {
  install.packages('BBmisc', dependencies = TRUE, INSTALL_opts = '--no-lock')
}
suppressPackageStartupMessages(require('BBmisc'))
# suppressPackageStartupMessages(require('rmsfuns'))

pkgs <- c('devtools', 'knitr', 'kableExtra', 'tidyr', 
          'readr', 'lubridate', 'data.table', 'reprex', 
          'timetk', 'plyr', 'dplyr', 'stringr', 'magrittr', 
          'tdplyr', 'tidyverse', 'formattable', 
          'echarts4r', 'paletteer')

suppressAll(lib(pkgs))
# load_pkg(pkgs)

## Set the timezone but not change the datetime
Sys.setenv(TZ = 'Asia/Tokyo')
## options(knitr.table.format = 'html') will set all kableExtra tables to be 'html', otherwise need to set the parameter on every single table.
options(warn = -1, knitr.table.format = 'html')#, digits.secs = 6)

## https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-abnor-in-rmd-but-not-in-r-script
knitr::opts_chunk$set(message = FALSE, warning = FALSE)#, 
                      #cache = TRUE, cache.lazy = FALSE)

rm(pkgs)
```

```{r error = TRUE}
dat <- fread('data/fuses.csv') %>% as.matrix
head(dat)
```

<span style='color:green;'>*Source : `r paste(dim(dat), collapse = ' x ')`*</span>

### Assignment

**Provide an MCMC algorithm to fit the mixture model**

$$
\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}
$$

```{r}
## Load package
if(!suppressPackageStartupMessages(require('rinvgamma'))) {
  ##install.packages('rinvgamma', dependencies = TRUE, INSTALL_opts = '--no-lock')
  devtools::install_github("dkahle/invgamma")
}

##function in 'rinvgamma' will be more accurate than 'MCMCpack'
suppressPackageStartupMessages(require('rinvgamma'))
#lib('MCMCpack')

dat = fread('data/fuses.csv') 
fuses = dat$V1
logfuses = log(fuses)

## Initialize the parameters
KK         = 2                               # number of components
n = length(fuses)                         # number of samples
v = array(0, dim=c(n,KK))
v[,1] = 0.5                    #Assign half weight to first component
v[,2] = 1-v[,1]                #Assign all of the remaining weights to the second component
mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
lambda = 1.0/mean1            #parameter for the first component
mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
sigmasquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
sigma = sqrt(sigmasquared)
w = mean(v[,1])
#print(paste(lambda, mu, tausquared, tau, w))
#print(paste(lambda, mu, sigmasquared, sigma, w))
paste(lambda, mu, sigmasquared, sigma, w)

# Priors
aa  = rep(1,KK)  # Uniform prior on 
# conjugate prior for exponential:
alpha = 2 #For Gamma distribution prior number of obs = alpha-1
beta = 1 #Sum of observations = beta
# conjugate prior for the normal
eta = mu          # Mean for the prior on mu
tau = 5          # Standard deviation 5 on the prior for mu_l
dd  = 2          # variance prior
qq  = 1

# Number of iterations of the sampler
rrr   = 6000
burn  = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n))
w.out     = rep(0, rrr)
lambda.out = rep(0,rrr)
mu.out    = array(0, rrr)
sigma.out = rep(0, rrr)
logpost   = rep(0, rrr)

# MCMC iterations
for(s in 1:rrr){
  # Sample the indicators
  cc = rep(0,n)
  for(i in 1:n){
    v = rep(0,KK)
    v[1] = log(w) + dexp(fuses[i], lambda, log=TRUE)  #Compute the log of the weights
    v[2] = log(1-w) + dnorm(logfuses[i], mu, sigma, log=TRUE)  #Compute the log of the weights
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = rbeta(1, aa[1] + sum(cc==1), aa[2] + sum(cc==2))
  
  # Sample the lambda
  nk    = sum(cc==1)
  xsumk = sum(fuses[cc==1])
  alpha.hat = alpha + nk
  beta.hat = beta + xsumk
  lambda = rgamma(1, shape = alpha.hat, rate = beta.hat)
  
  # Sample mu, the mean for norm
  nk    = sum(cc==2)
  xsumk = sum(logfuses[cc==2])
  tau2.hat = 1/(nk/sigma^2 + 1/tau^2)
  mu.hat  = tau2.hat*(xsumk/sigma^2 + eta/tau^2)
  mu   = rnorm(1, mu.hat, sqrt(tau2.hat))
  
  # Sample the variances
  dd.star = dd + nk/2
  qq.star = qq
  for(i in 1:n) {
    if (cc[i]==2) {
      qq.star = qq.star + ((logfuses[i] - mu)^2)/2
    }
  }
  sigma = sqrt(invgamma::rinvgamma(1, dd.star, qq.star))
  
  # Store samples
  cc.out[s,]    = cc
  w.out[s]     = w
  lambda.out[s] = lambda
  mu.out[s]    = mu
  sigma.out[s] = sigma
  
  
  for(i in 1:n){
    if(cc[i]==1){
      logpost[s] = logpost[s] + log(w) + dexp(fuses[i], lambda, log=TRUE)
    }else{
      logpost[s] = logpost[s] + log(1-w) + dnorm(logfuses[i], mu, sigma, log=TRUE)
    }
  }
  logpost[s] = logpost[s] + dbeta(w, aa[1], aa[2],log = T)
  logpost[s] = logpost[s] + dgamma(lambda, shape = alpha, rate = beta, log = T)
  logpost[s] = logpost[s] + dnorm(mu, eta, tau, log = T)
  logpost[s] = logpost[s] + log(invgamma::dinvgamma(sigma^2, dd, 1/qq))
  
  if(s/500==floor(s/500)){
    print(paste("s =",s, w, lambda, mu, sigma, logpost[s]))
  }
}

## Plot the logposterior distribution for various samples
par(mfrow=c(1,1))
par(mar=c(4,4,1,1)+0.1)
plot(logpost, type="l", xlab="Iterations", ylab="Log posterior")

w_avg = sum(w.out[(burn+1):rrr])/(rrr-burn)
lambda_avg = sum(lambda.out[(burn+1):rrr])/(rrr-burn)
mu_avg = sum(mu.out[(burn+1):rrr])/(rrr-burn)
sigma_avg = sum(sigma.out[(burn+1):rrr])/(rrr-burn)
```

```{r results = 'asis'}
#print(paste(w_avg, lambda_avg, mu_avg, sigma_avg))
dfm <- tibble(
  Category = c('weight', 'lambda', 'mean', 'sigma'), 
  Value = c(w_avg, lambda_avg, mu_avg, sigma_avg))
dfm %>% 
  kbl(caption = 'Summary', escape = FALSE) %>% 
  row_spec(0, background = 'DimGrey', color = 'gold', bold = TRUE) %>% 
  column_spec(1, background = 'CornflowerBlue') %>% 
  column_spec(2, background = 'DarkGrey') %>% 
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>% 
  kable_material(full_width = FALSE)
```

```{r results = 'asis'}
w_sum = 0
lambda_sum = 0
mu_sum = 0
sigma_sum = 0
counter = 0
for(s in 1:(rrr-burn)){
  w_sum = w_sum + w.out[s+burn]
  lambda_sum = lambda_sum + lambda.out[s+burn]
  mu_sum = mu_sum +mu.out[s+burn]
  sigma_sum = sigma_sum + sigma.out[s+burn]
  counter = counter + 1
}

#print(paste(counter, w_sum/counter, lambda_sum/counter, mu_sum/counter, sigma_sum/counter))
dfm <- tibble(
  Category = c('Counter', 'mean_weight', 'mean_lambda', 'mean', 'mean_sigma'), 
  Value = c(counter, w_sum/counter, lambda_sum/counter, mu_sum/counter, sigma_sum/counter))

dfm %>% 
  kbl(caption = 'Summary', escape = FALSE) %>% 
  row_spec(0, background = 'DimGrey', color = 'gold', bold = TRUE) %>% 
  column_spec(1, background = 'CornflowerBlue') %>% 
  column_spec(2, background = 'DarkGrey') %>% 
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>% 
  kable_material(full_width = FALSE)
```

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $\omega$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $\omega \le 1/2$.  For example, we could use $\omega = 0.1$, or simply sample a random starting from something like a Beta(1,9) distribution (which has expectation 0.1).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values. Alternatively, the values could be sampled from distributions centered around these values in order to make the starting values random.

Because the defective components should have shorter lifespan than normal components, it makes sense to take $\frac{1}{\lambda}$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

Finally, in this case the indicators $c_1, \ldots, c_n$ could be randomly initialized to either component of the mixture.

In summary, you should expect an initialization such as this one:

```{r error = TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
cc     = sample(1:2, n, TRUE, c(1/2, 1/2))
```

- 0ÁÇπ No
- 1ÁÇπ Some are, but not all
- 2ÁÇπ Yes

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto w \lambda \exp\left\{ -\lambda x_i \right\}$ while $\Pr(c_i = 2 \mid \cdots) \propto  (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x_i) - \mu \right)^2\right\}$

Hence, the code to sample the indicators might look something like this:

```{r error = TRUE}
# Full conditional for cc
v = rep(0,2)
for(i in 1:n){
  v[1]  = log(w) + dexp(x[i], lambda, log=TRUE)
  v[2]  = log(1-w) + dlnorm(x[i], mu, tau, log=TRUE)
  v     = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.  For example, the calculation could be vectorized to increase efficiency.

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\omega$ correct?**

This is a simple one, as its structure is common to all mixture models. Recalling that the prior on $\omega$ is a uniform distribution on [0,1][0,1], we have

$$
\omega \mid \cdots \sim \text{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)
$$

where $m(\mathbf{c})$ is the number of observations that \mathbf{c}c assigns to component 1. The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0ÁÇπ No
- 1ÁÇπ Some are correct, but not all of them
- 2ÁÇπ Yes

**Is the full conditional for the weight $\lambda$ correct?**

Since the prior is conditionally conjugate, it is easy to see that

$$
\lambda \mid \cdots \sim \text{Gam} \left( 1 + m(\mathbf{c}) , 1 + \sum_{i:c_i = 1} x_i\right)
$$

Hence, the code might look something like

```{r error=TRUE}
# Full conditional for lambda
lambda = rgamma(1, 1 + sum(cc==1), 1 + sum(x[cc==1]))
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\mu$ correct?**

The full-conditional for $\mu$ is a Gaussian distribution,

$$
\mu \mid \cdots \sim \text{N} \left( \frac{\frac{\sum_{i:c_i=1}x_i}{\tau^2} + 0}{\frac{m(\mathbf{c})}{\tau^2} + 1} , \frac{1}{\frac{m(\mathbf{c})}{\tau^2} + 1} \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for mu
mean.post = (sum(log(x[cc==2]))/tau^2 + 0)/(sum(cc==2)/tau^2 + 1)
std.post = sqrt(1/(sum(cc==2)/tau^2 + 1))
mu = rnorm(1, mean.post, std.post)
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\tau$ correct?**

The full conditional for $\tau^2$ is an inverse Gamma distribution,

$$
\tau^2 \mid \cdots \sim \\\text{IGam}\left( 2 + n - m\left(\mathbf{c}\right), 1+ \frac{1}{2}\sum_{i:c_i = 2} \left\{ \log x_i - \mu \right\}^2 \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for tau
tau = sqrt(1/rgamma(1, 2 + sum(cc==2), 1 + 0.5*sum((log(x[cc==2]) - mu)^2)))
```

- 0ÁÇπ No
- 1ÁÇπ Yes

<br><br>

## „É¨„Éì„É•„Éº

### 1st Peer

#### Assignment

**Provide an MCMC algorithm to fit the mixture model**

$$
\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}
$$

```{r error=TRUE}
rm(list=ls())
lib('MCMCpack')

Fuses <- read.csv(file = "data/fuses.csv")

## Initialize the parameters
# Number of iterations of the sampler
rrr   = 6000
burn  = 1000
set.seed(81196)

# Actual data
x  = Fuses$X1.062163
KK = 2

# nnumber of observations in data
n = nrow(Fuses)

# Storing the samples
cc.out    = array(0, dim=c(rrr, n))
w.out     = rep(0, rrr)
lambda.out    = array(0, dim=c(rrr, 1))
mu.out = array(0, dim = c(rrr,1))
tau.out = array(0, dim = c(rrr, 1))
logpost   = rep(0, rrr)

# MCMC iterations
for(s in 1:rrr){
  # Sample the indicators
  cc = rep(0,n)
  w.hat = runif(1)         
  lambda.hat = 8/mean(x)
  mu.hat = mean(log(x))
  tausquared.hat = var(log(x))

  for(i in 1:n){
    v = rep(0,KK)
    v[1] = log(w.hat) + dexp(x[i],lambda.hat,log = TRUE)
    v[2] = log(1 - w.hat) +  dlnorm(x[i], meanlog = mu.hat, sdlog = sqrt(tausquared.hat), log = TRUE) 
    
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w.hat = rbeta(1, 1 + sum(cc==1), 1 + sum(cc==2))  

  lambda.hat = rgamma(1, sum(cc==1) + 1, sum(x[cc==1]) + 1)
  
  tau_star = 1/(1 + sum(cc==2)/tausquared.hat)
  mu_star = tau_star * (sum(x[cc=2])/tausquared.hat)
  mu.hat = rnorm(1,mu_star, sqrt(tau_star))
  
  tausquared.hat = rinvgamma(1, 2 + 0.5*sum(cc==2),1 + 0.5*sum((log(x[cc==2])-mu.hat)^2))
  
  # Store samples
  cc.out[s,]   = cc
  w.out[s]     = w.hat
  lambda.out[s,]   = lambda.hat
  mu.out[s,] = mu.hat
  tau.out[s,] = sqrt(tausquared.hat)
  
  if(s/500==floor(s/500)){
    print(paste("s =",s))
  }
}
```

Provide you maximum likelihood estimates $E\left\{\omega \right\}$, $E\left\{\lambda \right\}$, $E\left\{\mu \right\}$ and $E\left\{\tau \right\}$ <ins>**rounded**</ins> to two decimal places.

```
mean(w.out[burn:rrr]) = 0.12
mean(lambda.out[burn:rrr]) = 2.28
mean(mu.out[burn:rrr]) = 0.01
mean(tau.out[burn:rrr]) = 0.86
```

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $\omega$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $\omega \le 1/2$.  For example, we could use $\omega = 0.1$, or simply sample a random starting from something like a Beta(1,9) distribution (which has expectation 0.1).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values. Alternatively, the values could be sampled from distributions centered around these values in order to make the starting values random.

Because the defective components should have shorter lifespan than normal components, it makes sense to take $\frac{1}{\lambda}$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

Finally, in this case the indicators $c_1, \ldots, c_n$ could be randomly initialized to either component of the mixture.

In summary, you should expect an initialization such as this one:

```{r error = TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
cc     = sample(1:2, n, TRUE, c(1/2, 1/2))
```

- 0ÁÇπ No
- <span style='color:green'>1ÁÇπ Some are, but not all</span>
- 2ÁÇπ Yes

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto w \lambda \exp\left\{ -\lambda x_i \right\}$ while $\Pr(c_i = 2 \mid \cdots) \propto  (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x_i) - \mu \right)^2\right\}$

Hence, the code to sample the indicators might look something like this:

```{r error = TRUE}
# Full conditional for cc
v = rep(0,2)
for(i in 1:n){
  v[1]  = log(w) + dexp(x[i], lambda, log=TRUE)
  v[2]  = log(1-w) + dlnorm(x[i], mu, tau, log=TRUE)
  v     = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.  For example, the calculation could be vectorized to increase efficiency.

- 0ÁÇπ No
- <span style='color:green'>1ÁÇπ Yes</span>

**Is the full conditional for the weight $\omega$ correct?**

This is a simple one, as its structure is common to all mixture models. Recalling that the prior on $\omega$ is a uniform distribution on [0,1][0,1], we have

$$
\omega \mid \cdots \sim \text{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)
$$

where $m(\mathbf{c})$ is the number of observations that \mathbf{c}c assigns to component 1. The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0ÁÇπ No
- 1ÁÇπ Some are correct, but not all of them
-<span style='color:green'> 2ÁÇπ Yes</span>

**Is the full conditional for the weight $\lambda$ correct?**

Since the prior is conditionally conjugate, it is easy to see that

$$
\lambda \mid \cdots \sim \text{Gam} \left( 1 + m(\mathbf{c}) , 1 + \sum_{i:c_i = 1} x_i\right)
$$

Hence, the code might look something like

```{r error=TRUE}
# Full conditional for lambda
lambda = rgamma(1, 1 + sum(cc==1), 1 + sum(x[cc==1]))
```

- 0ÁÇπ No
- <span style='color:green'>1ÁÇπ Yes</span>

**Is the full conditional for the weight $\mu$ correct?**

The full-conditional for $\mu$ is a Gaussian distribution,

$$
\mu \mid \cdots \sim \text{N} \left( \frac{\frac{\sum_{i:c_i=1}x_i}{\tau^2} + 0}{\frac{m(\mathbf{c})}{\tau^2} + 1} , \frac{1}{\frac{m(\mathbf{c})}{\tau^2} + 1} \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for mu
mean.post = (sum(log(x[cc==2]))/tau^2 + 0)/(sum(cc==2)/tau^2 + 1)
std.post = sqrt(1/(sum(cc==2)/tau^2 + 1))
mu = rnorm(1, mean.post, std.post)
```

- <span style='color:red'>0ÁÇπ No</span>
- 1ÁÇπ Yes

**Is the full conditional for the weight $\tau$ correct?**

The full conditional for $\tau^2$ is an inverse Gamma distribution,

$$
\tau^2 \mid \cdots \sim \\\text{IGam}\left( 2 + n - m\left(\mathbf{c}\right), 1+ \frac{1}{2}\sum_{i:c_i = 2} \left\{ \log x_i - \mu \right\}^2 \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for tau
tau = sqrt(1/rgamma(1, 2 + sum(cc==2), 1 + 0.5*sum((log(x[cc==2]) - mu)^2)))
```

- 0ÁÇπ No
- <span style='color:green'>1ÁÇπ Yes</span>

**Are the posterior means of the parameters generated by the algorithm correct?**

The posterior means, rounded to two decimal places, are $\text{E} \left\{ w \right\} \approx 0.10$, $\text{E} \left\{ \lambda \right\} \approx 2.29$, $\text{E} \left\{ \mu \right\} \approx 0.79$ and $\text{E} \left\{ \tau \right\} \approx 0.38$

- <span style='color:red'>0ÁÇπ No</span>
- 2ÁÇπ Yes

<br><br>

### 2nd Peer

#### Asignment

**Provide an MCMC algorithm to fit the mixture model**

$$
\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}
$$

```{r}
x  = as.matrix(read.csv("/data/fuses.csv",header=FALSE))

## Initialize the parameters
n  = length(x)
cc = rep(0, n) 
cc = sample(1:2, n, replace=T, prob=c(1/2, 1/2)) 
mu = mean(log(x)) 
sigma  = sd(log(x)) 
lambda = 1/mean(x) 
w      = 0.5  

#prior parameters
eta = 0          # Mean 0 for the prior on mu
tau = 1          # Standard deviation 1 on the prior for mu



```

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $\omega$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $\omega \le 1/2$.  For example, we could use $\omega = 0.1$, or simply sample a random starting from something like a Beta(1,9) distribution (which has expectation 0.1).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values. Alternatively, the values could be sampled from distributions centered around these values in order to make the starting values random.

Because the defective components should have shorter lifespan than normal components, it makes sense to take $\frac{1}{\lambda}$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

Finally, in this case the indicators $c_1, \ldots, c_n$ could be randomly initialized to either component of the mixture.

In summary, you should expect an initialization such as this one:

```{r error = TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
cc     = sample(1:2, n, TRUE, c(1/2, 1/2))
```

- 0ÁÇπ No
- 1ÁÇπ Some are, but not all
- 2ÁÇπ Yes

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto w \lambda \exp\left\{ -\lambda x_i \right\}$ while $\Pr(c_i = 2 \mid \cdots) \propto  (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x_i) - \mu \right)^2\right\}$

Hence, the code to sample the indicators might look something like this:

```{r error = TRUE}
# Full conditional for cc
v = rep(0,2)
for(i in 1:n){
  v[1]  = log(w) + dexp(x[i], lambda, log=TRUE)
  v[2]  = log(1-w) + dlnorm(x[i], mu, tau, log=TRUE)
  v     = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.  For example, the calculation could be vectorized to increase efficiency.

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\omega$ correct?**

This is a simple one, as its structure is common to all mixture models. Recalling that the prior on $\omega$ is a uniform distribution on [0,1][0,1], we have

$$
\omega \mid \cdots \sim \text{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)
$$

where $m(\mathbf{c})$ is the number of observations that \mathbf{c}c assigns to component 1. The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0ÁÇπ No
- 1ÁÇπ Some are correct, but not all of them
- 2ÁÇπ Yes

**Is the full conditional for the weight $\lambda$ correct?**

Since the prior is conditionally conjugate, it is easy to see that

$$
\lambda \mid \cdots \sim \text{Gam} \left( 1 + m(\mathbf{c}) , 1 + \sum_{i:c_i = 1} x_i\right)
$$

Hence, the code might look something like

```{r error=TRUE}
# Full conditional for lambda
lambda = rgamma(1, 1 + sum(cc==1), 1 + sum(x[cc==1]))
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\mu$ correct?**

The full-conditional for $\mu$ is a Gaussian distribution,

$$
\mu \mid \cdots \sim \text{N} \left( \frac{\frac{\sum_{i:c_i=1}x_i}{\tau^2} + 0}{\frac{m(\mathbf{c})}{\tau^2} + 1} , \frac{1}{\frac{m(\mathbf{c})}{\tau^2} + 1} \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for mu
mean.post = (sum(log(x[cc==2]))/tau^2 + 0)/(sum(cc==2)/tau^2 + 1)
std.post = sqrt(1/(sum(cc==2)/tau^2 + 1))
mu = rnorm(1, mean.post, std.post)
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\tau$ correct?**

The full conditional for $\tau^2$ is an inverse Gamma distribution,

$$
\tau^2 \mid \cdots \sim \\\text{IGam}\left( 2 + n - m\left(\mathbf{c}\right), 1+ \frac{1}{2}\sum_{i:c_i = 2} \left\{ \log x_i - \mu \right\}^2 \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for tau
tau = sqrt(1/rgamma(1, 2 + sum(cc==2), 1 + 0.5*sum((log(x[cc==2]) - mu)^2)))
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Are the posterior means of the parameters generated by the algorithm correct?**

The posterior means, rounded to two decimal places, are $\text{E} \left\{ w \right\} \approx 0.10$, $\text{E} \left\{ \lambda \right\} \approx 2.29$, $\text{E} \left\{ \mu \right\} \approx 0.79$ and $\text{E} \left\{ \tau \right\} \approx 0.38$

- 0ÁÇπ No
- 2ÁÇπ Yes

<br><br>

### 3rd Peer

#### Asignment

**Provide an MCMC algorithm to fit the mixture model**

$$
\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}
$$

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $\omega$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $\omega \le 1/2$.  For example, we could use $\omega = 0.1$, or simply sample a random starting from something like a Beta(1,9) distribution (which has expectation 0.1).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values. Alternatively, the values could be sampled from distributions centered around these values in order to make the starting values random.

Because the defective components should have shorter lifespan than normal components, it makes sense to take $\frac{1}{\lambda}$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

Finally, in this case the indicators $c_1, \ldots, c_n$ could be randomly initialized to either component of the mixture.

In summary, you should expect an initialization such as this one:

```{r error = TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
cc     = sample(1:2, n, TRUE, c(1/2, 1/2))
```

- 0ÁÇπ No
- 1ÁÇπ Some are, but not all
- 2ÁÇπ Yes

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto w \lambda \exp\left\{ -\lambda x_i \right\}$ while $\Pr(c_i = 2 \mid \cdots) \propto  (1- \\w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x_i) - \mu \right)^2\right\}$

Hence, the code to sample the indicators might look something like this:

```{r error = TRUE}
# Full conditional for cc
v = rep(0,2)
for(i in 1:n){
  v[1]  = log(w) + dexp(x[i], lambda, log=TRUE)
  v[2]  = log(1-w) + dlnorm(x[i], mu, tau, log=TRUE)
  v     = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.  For example, the calculation could be vectorized to increase efficiency.

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\omega$ correct?**

This is a simple one, as its structure is common to all mixture models. Recalling that the prior on $\omega$ is a uniform distribution on [0,1][0,1], we have

$$
\omega \mid \cdots \sim \text{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)
$$

where $m(\mathbf{c})$ is the number of observations that \mathbf{c}c assigns to component 1. The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0ÁÇπ No
- 1ÁÇπ Some are correct, but not all of them
- 2ÁÇπ Yes

**Is the full conditional for the weight $\lambda$ correct?**

Since the prior is conditionally conjugate, it is easy to see that

$$
\lambda \mid \cdots \sim \text{Gam} \left( 1 + m(\mathbf{c}) , 1 + \sum_{i:c_i = 1} x_i\right)
$$

Hence, the code might look something like

```{r error=TRUE}
# Full conditional for lambda
lambda = rgamma(1, 1 + sum(cc==1), 1 + sum(x[cc==1]))
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\mu$ correct?**

The full-conditional for $\mu$ is a Gaussian distribution,

$$
\mu \mid \cdots \sim \text{N} \left( \frac{\frac{\sum_{i:c_i=1}x_i}{\tau^2} + 0}{\frac{m(\mathbf{c})}{\tau^2} + 1} , \frac{1}{\frac{m(\mathbf{c})}{\tau^2} + 1} \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for mu
mean.post = (sum(log(x[cc==2]))/tau^2 + 0)/(sum(cc==2)/tau^2 + 1)
std.post = sqrt(1/(sum(cc==2)/tau^2 + 1))
mu = rnorm(1, mean.post, std.post)
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Is the full conditional for the weight $\tau$ correct?**

The full conditional for $\tau^2$ is an inverse Gamma distribution,

$$
\tau^2 \mid \cdots \sim \\\text{IGam}\left( 2 + n - m\left(\mathbf{c}\right), 1+ \frac{1}{2}\sum_{i:c_i = 2} \left\{ \log x_i - \mu \right\}^2 \right)
$$

The corresponding R code might look like:

```{r error=TRUE}
# Full conditional for tau
tau = sqrt(1/rgamma(1, 2 + sum(cc==2), 1 + 0.5*sum((log(x[cc==2]) - mu)^2)))
```

- 0ÁÇπ No
- 1ÁÇπ Yes

**Are the posterior means of the parameters generated by the algorithm correct?**

The posterior means, rounded to two decimal places, are $\text{E} \left\{ w \right\} \approx 0.10$, $\text{E} \left\{ \lambda \right\} \approx 2.29$, $\text{E} \left\{ \mu \right\} \approx 0.79$ and $\text{E} \left\{ \tau \right\} \approx 0.38$

- 0ÁÇπ No
- 2ÁÇπ Yes

- 0ÁÇπ No
- 1ÁÇπ Yes

<br><br>

## „Éá„Ç£„Çπ„Ç´„ÉÉ„Ç∑„Éß„É≥

<br><br>

# Appendix

## Blooper

Here I noticed the function from `rinvgamma` and `MCMCpack` get a slight different result where `rinvgamma` will be more accurate.

![example from `MCMCpack`](figure/MCMCpack_example.png)

![example from `rinvgamma`](figure/rinvgamma_example.png)


## Documenting File Creation 

It's useful to record some information about how your file was created.

- File creation date: 2021-05-12
- File latest updated date: `r today('Asia/Tokyo')`
- `r R.version.string`
- [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
- File version: 1.0.0
- Author Profile: [¬ÆŒ≥œÉ, Eng Lian Hu](https://github.com/scibrokes/owner)
- GitHub: [Source Code](https://github.com/englianhu/coursera-bayesian-statistics-mixture-models)
- Additional session information:

```{r info, warning = FALSE, results = 'asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('magrittr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))
suppressMessages(require('knitr', quietly = TRUE))
suppressMessages(require('kableExtra', quietly = TRUE))

sys1 <- devtools::session_info()$platform %>% 
  unlist %>% data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL

sys2 <- data.frame(Sys.info()) %>% 
  dplyr::mutate(Category = rownames(.)) %>% .[2:1]
names(sys2)[2] <- c('Sys.info')
rownames(sys2) <- NULL

if (nrow(sys1) == 9 & nrow(sys2) == 8) {
  sys2 %<>% rbind(., data.frame(
  Category = 'Current time', 
  Sys.info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JSTüóæ')))
} else {
  sys1 %<>% rbind(., data.frame(
  Category = 'Current time', 
  session_info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JSTüóæ')))
}

sys <- cbind(sys1, sys2) %>% 
  kbl(caption = 'Additional session information:') %>% 
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>% 
  row_spec(0, background = 'DimGrey', color = 'yellow') %>% 
  column_spec(1, background = 'CornflowerBlue', color = 'red') %>% 
  column_spec(2, background = 'grey', color = 'black') %>% 
  column_spec(3, background = 'CornflowerBlue', color = 'blue') %>% 
  column_spec(4, background = 'grey', color = 'white') %>% 
  row_spec(9, bold = T, color = 'yellow', background = '#D7261E')

rm(sys1, sys2)
sys
```

## Reference

- [Is it possible to change the hover colour in kableExtra?](https://stackoverflow.com/a/49429306/3806250)
- []()

<br><br>
