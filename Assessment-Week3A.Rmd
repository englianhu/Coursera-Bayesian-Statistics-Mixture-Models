---
title: "<img src='figure/coursera.jpg' width='37'> <img src='figure/ucsc.png' width='240'>"
subtitle: "<span style='color:white; background-color:#4E79A7;'>Bayesian Statistics: Mixture Models</span> (Assessment Week3 A Codes)"
author: "[®γσ, Lian Hu](https://englianhu.github.io/) <img src='figure/quantitative trader 1.jpg' width='12'> <img src='figure/ENG.jpg' width='24'> ®"
date: "`r lubridate::today('Asia/Tokyo')`"
output:
  html_document: 
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    css: CSSBackgrounds.css
---

<br>
<span style='color:green'>**Theme Song**</span>
<br>

<audio src="music/California-Dreaming-Chorus.mp3" controls></audio>
<br>

------

# Setting

## SCSS Setup

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
.table-hover > tbody > tr:hover { 
  background-color: #8D918D;
}
</style>

```{r class.source = 'bg-success', class.output = 'bg-primary', message = FALSE, warning = FALSE}
#if(!suppressPackageStartupMessages(require('BBmisc'))) {
#  install.packages('BBmisc', dependencies = TRUE, INSTALL_opts = '--no-lock')
#}
suppressPackageStartupMessages(library('BBmisc'))
#remotes::install_github("rstudio/sass")
#lib('sass')
suppressPackageStartupMessages(library('sass'))
```

```{scss class.source = 'bg-success', class.output = 'bg-primary'}
/* https://stackoverflow.com/a/66029010/3806250 */
h1 { color: #002C54; }
h2 { color: #2F496E; }
h3 { color: #375E97; }
h4 { color: #375E97; }
h5 { color: #375E97; }

/* ----------------------------------------------------------------- */
/* https://gist.github.com/himynameisdave/c7a7ed14500d29e58149#file-broken-gradient-animation-less */
.hover01 {
  /* color: #FFD64D; */
  background: linear-gradient(155deg, #EDAE01 0%, #FFEB94 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #EDAE01 20%, #FFEB94 80%);
    }
  }

.hover02 {
  color: #FFD64D;
  background: linear-gradient(155deg, #002C54 0%, #4CB5F5 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #002C54 20%, #4CB5F5 80%);
    }
  }

.hover03 {
  color: #FFD64D;
  background: linear-gradient(155deg, #A10115 0%, #FF3C5C 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #A10115 20%, #FF3C5C 80%);
    }
  }
```

```{r global_options, class.source = 'hover01', class.output = 'hover02'}
## https://stackoverflow.com/a/36846793/3806250
options(width = 999)
knitr::opts_chunk$set(class.source = 'hover01', class.output = 'hover02', class.error = 'hover03')
```

<br><br>

## Setup

```{r warning=FALSE, message=FALSE}
pkgs <- c('devtools', 'knitr', 'kableExtra', 'tidyr', 
          'readr', 'lubridate', 'data.table', 'reprex', 
          'timetk', 'plyr', 'dplyr', 'stringr', 'magrittr', 
          'tdplyr', 'tidyverse', 'formattable', 
          'echarts4r', 'paletteer')

suppressAll(lib(pkgs))
# load_pkg(pkgs)

## Set the timezone but not change the datetime
Sys.setenv(TZ = 'Asia/Tokyo')
## options(knitr.table.format = 'html') will set all kableExtra tables to be 'html', otherwise need to set the parameter on every single table.
options(warn = -1, knitr.table.format = 'html')#, digits.secs = 6)

## https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-abnor-in-rmd-but-not-in-r-script
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
#, 
                      #cache = TRUE, cache.lazy = FALSE)

rm(pkgs)
```

<br><br>

# 受講生によるテスト：The MCMC algorithm for zero-inflated mixtures

5月17日 15:59 JST までに提出

**課題を受ける準備はできていますか？**

以下に提出のための指示が表示されます。

<br><br>

## 説明

This assignment is similar to one you already completed in Lesson 3. However, in this case you are asked to use an MCMC algorithm to perform Bayesian inference on the model parameters, instead of an EM algorithm to find maximum likelihood estimators.

A biologist is interest in characterizing the number of eggs laid by a particular bird species. To do this, they sample $n=300$ nests on a site in Southern California. The observations are contained in the attached file `nestsize.csv`:

[nestsize.csv](https://github.com/englianhu/Coursera-Bayesian-Statistics-Mixture-Models/blob/main/data/nestsize.csv)

The following graph compares the empirical distribution of the data against a Poison distribution whose parameter has been set to its maximum likelihood estimator:

![](figure/zeroinflated.png)

As you can see, the Poisson distribution underestimates the number of empty nests in the data, and overestimates the number of nests with either 1 or 2 eggs.  To address this, you are asked to modify the implementation of the MCMC algorithm contained in the Reading "Sample code for MCMC example 1" so that you can fit a mixture between a point mass at zero and a Poisson distribution (we call this a "zero-inflated Poisson" distribution):

$f(x) = w \delta_0(x) + (1-w) \frac{e^{-\lambda} \lambda^x}{x!} \quad \quad x \in \{0,1,2,\ldots\}$

where $\delta_0(x)$ represents the degenerate distribution placing all of its mass at zero. You then should run your algorithm for $5,000$ iterations (after a burn-in period of $1,000$ iterations) with the data contained in `nestsize.csv` and report your estimates of the posterior means, rounded to two decimal places.

In carrying out this assignment assume the following priors for your unknown parameters: $\omega \sim Uni[0,1]$ and $\lambda \sim Exp(1)$.

<br><br>

### Review criteria

The code you generate should follow the same structure as "Sample code for MCMC example 1". In particular, focus on a Gibss sampler that alternates between the full conditionals for $\omega$, $\lambda$ and the latent component indicators $c_1, \ldots, c_n$.  Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect the fact that (1) parameters have been initialized in a reasonable way, (2) each of the two full conditional distributions associated with the sampler are correct, and (2) the numerical values that you obtain are correct.  To simplify the peer-review process, assume that component 1 corresponds to the point mass at zero, while component 2 corresponds to the Poisson distribution.

<br><br>

## 自分の提出物

### Assignment

```{r error=TRUE}
KK        = 2
w         = 1/2
mu        = mean(x)
n         = length(x)
aa        = rep(1,KK)  # Uniform prior on w
maxv      = 1 - 1e-6
# Number of iterations of the sampler
rrr       = 6000
burn      = 1000
w_out     = rep(0, rrr)
mu_out    = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  cc      = rep(0,n)
  for(i in 1:n){
    v     = rep(0,KK)
    v[1]  = log(w)
    #v[1]  = log(w) + dunif(x[i], max=maxv, log=T) # treat first component as uniform (0, .9999)
    v[2]  = log(1-w) + dpois(x[i], mu, log=T)
    v     = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=T, prob=v)
  }
  
  # Sample the weights
  w = rbeta(1, aa[1] + sum(cc==1), aa[2] + sum(cc==2))

  # Sample the mean
  for(k in 2:KK){
    nk    = sum(cc==k)
    xsumk = sum(x[cc==k])
    mu    = rgamma(1, xsumk + 1, nk + 1)
  }

  # Store samples
  w_out[s]    = w
  mu_out[s]   = mu
}
```

```{r error=TRUE}
mean(mu_out[burn:rrr])# = 3.051
mean(w_out[burn:rrr])# = 0.398
```


<br><br>

### Marking


<br><br>

## ピアレビュー

### 1st Peer

#### Asignment

Provide a Markov chain Monte Carlo algorithm to fit a zero-inflated Poisson distribution.

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=TRUE, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2 # Full conditional for cc

for(i in 1:n){
   v = rep(0,2)
   
   if(x[i]==0){
     v[1] = log(w)
     v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
     v    = exp(v - max(v))/sum(exp(v - max(v)))
     
    }else{
      v[1] = 0
      v[2] = 1
    }
   cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)

w
lambda
```

Provide you estimates of the posterior means $\mbox{E}\left(\lambda \mid \mathbf{x} \right)$ and $\mbox{E}\left(w \mid \mathbf{x} \right)$.

```{r error=TRUE}
mean(mu_out[burn:rrr])# = 3.051
mean(w_out[burn:rrr])# = 0.398
```

#### Marking

**Are the parameters initialized in a reasonable way?**

Correctly initializing the indicators $c_1, \ldots, c_n$ is key. In particular, observations such that $x_i \neq 0$ must have $c_i = 2$. As for observations for which $x_i = 0$, these can be randomly initialized to either component.

For the other parameters, it is reasonable to initialize $\lambda$ to the mean of the observations and the weight $\omega$ to a relatively small value (as we did in the EM algorithm).

Hence, the code for the initialization of the algorithm might look something like:

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=T, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto \begin{cases} w & x_i=0 \\ 0 & \mbox{otherwise} \end{cases}$

while

$\Pr(c_i = 2 \mid \cdots) \propto \begin{cases} (1-w) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} & x_i=0 \\ 1 & \mbox{otherwise} \end{cases}$

Hence, the code to sample the indicators might look something like this:

```{r error=TRUE}
# Full conditional for cc
for(i in 1:n){
  v = rep(0,2)
  if(x[i]==0){
    v[1] = log(w)
    v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
    v    = exp(v - max(v))/sum(exp(v - max(v)))
  }else{
    v[1] = 0
    v[2] = 1
  }
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the weight ww correct?**

This is a simple one, as its structure is common to all mixture models.  Recalling that the prior on $\omega$ is a uniform distribution on [0,1], we have

$\omega \mid \cdots \sim \mbox{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)$

where $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1.  The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the rate $\lambda$ correct?**

Because we use a exponential prior on $\lambda$, the full conditional posterior is a Gamma distribution,

$\lambda \mid \cdots \sim \mbox{Gamma}\left( 1 + \sum_{i : c_i = 2} x_i , 1 + n-m(\mathbf{c}) \right)$

where, as before, $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1 (so that $n - m(\mathbf{c})$ is the number of observations assigned to component 2), and $\sum_{c_i = 2}^{i} x_i$ is the sum of the observations assigned to component 2.

Hence, the code for this prompt might look something like:

```{r error=TRUE}
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
```

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Are the posterior means generated by the algorithm correct?**

Recall that the posterior means can be approximated by simple averages of the posterior samples obtained after burn in.  While there might a bit of Monte Carlo error in the answers reported, the values should be $E(\lambda∣x)≈3.05$ and $E(\omega∣x)≈0.40$. Note that these values are very close to those generated by the EM algorithm.

- <span style='color:red;'>0点 No</span>
- 1点 Yes

<br>

### 2nd Peer

#### Asignment

Provide a Markov chain Monte Carlo algorithm to fit a zero-inflated Poisson distribution.

```{r error=TRUE}
KK        = 2
w         = 1/2
mu        = mean(x)
n         = length(x)
aa        = rep(1,KK)  # Uniform prior on w
maxv      = 1 - 1e-6
# Number of iterations of the sampler
rrr       = 6000
burn      = 1000
w_out     = rep(0, rrr)
mu_out    = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  cc      = rep(0,n)
  for(i in 1:n){
    v     = rep(0,KK)
    v[1]  = log(w)
    #v[1]  = log(w) + dunif(x[i], max=maxv, log=T) # treat first component as uniform (0, .9999)
    v[2]  = log(1-w) + dpois(x[i], mu, log=T)
    v     = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=T, prob=v)
  }
  
  # Sample the weights
  w = rbeta(1, aa[1] + sum(cc==1), aa[2] + sum(cc==2))

  # Sample the mean
  for(k in 2:KK){
    nk    = sum(cc==k)
    xsumk = sum(x[cc==k])
    mu    = rgamma(1, xsumk + 1, nk + 1)
  }

  # Store samples
  w_out[s]    = w
  mu_out[s]   = mu
}
```

Provide you estimates of the posterior means $\mbox{E}\left(\lambda \mid \mathbf{x} \right)$ and $\mbox{E}\left(w \mid \mathbf{x} \right)$.

```{r error=TRUE}
mean(mu_out[burn:rrr])# = 3.051
mean(w_out[burn:rrr])# = 0.398
```

#### Marking

**Are the parameters initialized in a reasonable way?**

Correctly initializing the indicators $c_1, \ldots, c_n$ is key. In particular, observations such that $x_i \neq 0$ must have $c_i = 2$. As for observations for which $x_i = 0$, these can be randomly initialized to either component.

For the other parameters, it is reasonable to initialize $\lambda$ to the mean of the observations and the weight $\omega$ to a relatively small value (as we did in the EM algorithm).

Hence, the code for the initialization of the algorithm might look something like:

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=T, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto \begin{cases} w & x_i=0 \\ 0 & \mbox{otherwise} \end{cases}$

while

$\Pr(c_i = 2 \mid \cdots) \propto \begin{cases} (1-w) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} & x_i=0 \\ 1 & \mbox{otherwise} \end{cases}$

Hence, the code to sample the indicators might look something like this:

```{r error=TRUE}
# Full conditional for cc
for(i in 1:n){
  v = rep(0,2)
  if(x[i]==0){
    v[1] = log(w)
    v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
    v    = exp(v - max(v))/sum(exp(v - max(v)))
  }else{
    v[1] = 0
    v[2] = 1
  }
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the weight ww correct?**

This is a simple one, as its structure is common to all mixture models.  Recalling that the prior on $\omega$ is a uniform distribution on [0,1], we have

$\omega \mid \cdots \sim \mbox{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)$

where $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1.  The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the rate $\lambda$ correct?**

Because we use a exponential prior on $\lambda$, the full conditional posterior is a Gamma distribution,

$\lambda \mid \cdots \sim \mbox{Gamma}\left( 1 + \sum_{i : c_i = 2} x_i , 1 + n-m(\mathbf{c}) \right)$

where, as before, $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1 (so that $n - m(\mathbf{c})$ is the number of observations assigned to component 2), and $\sum_{c_i = 2}^{i} x_i$ is the sum of the observations assigned to component 2.

Hence, the code for this prompt might look something like:

```{r error=TRUE}
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
```

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Are the posterior means generated by the algorithm correct?**

Recall that the posterior means can be approximated by simple averages of the posterior samples obtained after burn in.  While there might a bit of Monte Carlo error in the answers reported, the values should be $E(\lambda∣x)≈3.05$ and $E(\omega∣x)≈0.40$. Note that these values are very close to those generated by the EM algorithm.

- <span style='color:red;'>0点 No</span>
- 1点 Yes

<br>

### 3rd Peer

#### Asignment

Provide a Markov chain Monte Carlo algorithm to fit a zero-inflated Poisson distribution.

```{r error=TRUE}
## Initialize the parameters
KK     = 2                               # number of components
n      = length(x)                         # number of samples
v      = array(0, dim=c(n,KK))
v[,1]  = 0.5*(x==0)                    #Assign half weight for nests with 0 eggs to first component
v[,2]  = 1-v[,1]                              #Assign all of the remaining nests to the second component
w      = mean(v[,1])                          #weight of the first component
lambda = sum(x*v[,2])/sum(v[,2])     #parameter (mean) of the second component

## The actual MCMC algorithm starts here
# Priors
aa     = rep(1,KK)  # Uniform prior on w
alpha  = 2 #For Gamma distribution prior number of obs = alpha-1
beta   = 1 #Sum of observations = beta

# Number of iterations of the sampler
rrr    = 6000
burn   = 1000

# Storing the samples
cc.out      = array(0, dim=c(rrr, n))
w.out       = rep(0, rrr)
lambda.out  = array(0, dim=c(rrr, 1))
logpost     = rep(0, rrr)

# MCMC iterations
# MCMC iterations
for(s in 1:rrr){
  # Sample the indicators
  cc = rep(0,n)
  for(i in 1:n){
    v       = rep(0,KK)
    if (x[i]==0) {
      v[1]  = (w/(w+(1-w)*exp(-lambda)))
    } else {
      v[1]  = 0.0
    }
    v[2]    = 1.0 - v[1]
    cc[i]   = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w         = rbeta(1, aa[1] + sum(cc==1), aa[2] + sum(cc==2))
  
  # Sample lambda
  nk        = sum(cc==2)
  xsumk     = sum(x[cc==2])
  alpha.hat = alpha + xsumk
  beta.hat  = beta + nk
  lambda    = rgamma(1, shape = alpha.hat, rate = beta.hat)
  
  # Store samples
  cc.out[s,]     = cc
  w.out[s]       = w
  lambda.out[s,] = lambda
  for(i in 1:n){
    if(cc[i]==1){
      logpost[s] = logpost[s] + log(w)
    }else{
      logpost[s] = logpost[s] + log(1-w) + dpois(x[i], lambda, log=TRUE)
    }
  }
  logpost[s] = logpost[s] + dbeta(w, aa[1], aa[2],log = T)
  logpost[s] = logpost[s] + dgamma(lambda, shape = alpha, rate = beta)
  if(s/500==floor(s/500)){
    print(paste("s =",s, w, lambda, logpost[s]))
  }
}


## Plot the logposterior distribution for various samples
par(mfrow=c(1,1))
par(mar=c(4,4,1,1)+0.1)
plot(logpost, type="l", xlab="Iterations", ylab="Log posterior")

w_avg = sum(w.out[(burn+1):rrr])/(rrr-burn)
lambda_avg = sum(lambda.out[(burn+1):rrr])/(rrr-burn)
print(paste(w_avg, lambda_avg))

w_sum = 0
lambda_sum = 0
counter = 0
for(s in 1:(rrr-burn)){
  w_sum = w_sum + w.out[s+burn]
  lambda_sum = lambda_sum + lambda.out[s+burn]
  counter = counter + 1
}
print(paste(counter, w_sum/counter, lambda_sum/counter))
```

Provide you estimates of the posterior means $\mbox{E}\left(\lambda \mid \mathbf{x} \right)$ and $\mbox{E}\left(w \mid \mathbf{x} \right)$.

```{r error=TRUE}
#E(lambda|x) = 3.056 and E(w|x) = 0.398
mean(lambda_sum)
mean(w_sum)
```

#### Marking

**Are the parameters initialized in a reasonable way?**

Correctly initializing the indicators $c_1, \ldots, c_n$ is key. In particular, observations such that $x_i \neq 0$ must have $c_i = 2$. As for observations for which $x_i = 0$, these can be randomly initialized to either component.

For the other parameters, it is reasonable to initialize $\lambda$ to the mean of the observations and the weight $\omega$ to a relatively small value (as we did in the EM algorithm).

Hence, the code for the initialization of the algorithm might look something like:

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=T, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto \begin{cases} w & x_i=0 \\ 0 & \mbox{otherwise} \end{cases}$

while

$\Pr(c_i = 2 \mid \cdots) \propto \begin{cases} (1-w) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} & x_i=0 \\ 1 & \mbox{otherwise} \end{cases}$

Hence, the code to sample the indicators might look something like this:

```{r error=TRUE}
# Full conditional for cc
for(i in 1:n){
  v = rep(0,2)
  if(x[i]==0){
    v[1] = log(w)
    v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
    v    = exp(v - max(v))/sum(exp(v - max(v)))
  }else{
    v[1] = 0
    v[2] = 1
  }
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the weight ww correct?**

This is a simple one, as its structure is common to all mixture models.  Recalling that the prior on $\omega$ is a uniform distribution on [0,1], we have

$\omega \mid \cdots \sim \mbox{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)$

where $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1.  The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the rate $\lambda$ correct?**

Because we use a exponential prior on $\lambda$, the full conditional posterior is a Gamma distribution,

$\lambda \mid \cdots \sim \mbox{Gamma}\left( 1 + \sum_{i : c_i = 2} x_i , 1 + n-m(\mathbf{c}) \right)$

where, as before, $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1 (so that $n - m(\mathbf{c})$ is the number of observations assigned to component 2), and $\sum_{c_i = 2}^{i} x_i$ is the sum of the observations assigned to component 2.

Hence, the code for this prompt might look something like:

```{r error=TRUE}
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
```

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Are the posterior means generated by the algorithm correct?**

Recall that the posterior means can be approximated by simple averages of the posterior samples obtained after burn in.  While there might a bit of Monte Carlo error in the answers reported, the values should be $E(\lambda∣x)≈3.05$ and $E(\omega∣x)≈0.40$. Note that these values are very close to those generated by the EM algorithm.

- <span style='color:red;'>0点 No</span>
- 1点 Yes

<br>

### 4th Peer

#### Asignment

Provide a Markov chain Monte Carlo algorithm to fit a zero-inflated Poisson distribution.

```{r error=TRUE}

```

Provide you estimates of the posterior means $\mbox{E}\left(\lambda \mid \mathbf{x} \right)$ and $\mbox{E}\left(w \mid \mathbf{x} \right)$.

```{r error=TRUE}

```

#### Marking

**Are the parameters initialized in a reasonable way?**

Correctly initializing the indicators $c_1, \ldots, c_n$ is key. In particular, observations such that $x_i \neq 0$ must have $c_i = 2$. As for observations for which $x_i = 0$, these can be randomly initialized to either component.

For the other parameters, it is reasonable to initialize $\lambda$ to the mean of the observations and the weight $\omega$ to a relatively small value (as we did in the EM algorithm).

Hence, the code for the initialization of the algorithm might look something like:

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=T, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto \begin{cases} w & x_i=0 \\ 0 & \mbox{otherwise} \end{cases}$

while

$\Pr(c_i = 2 \mid \cdots) \propto \begin{cases} (1-w) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} & x_i=0 \\ 1 & \mbox{otherwise} \end{cases}$

Hence, the code to sample the indicators might look something like this:

```{r error=TRUE}
# Full conditional for cc
for(i in 1:n){
  v = rep(0,2)
  if(x[i]==0){
    v[1] = log(w)
    v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
    v    = exp(v - max(v))/sum(exp(v - max(v)))
  }else{
    v[1] = 0
    v[2] = 1
  }
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the weight ww correct?**

This is a simple one, as its structure is common to all mixture models.  Recalling that the prior on $\omega$ is a uniform distribution on [0,1], we have

$\omega \mid \cdots \sim \mbox{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)$

where $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1.  The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the rate $\lambda$ correct?**

Because we use a exponential prior on $\lambda$, the full conditional posterior is a Gamma distribution,

$\lambda \mid \cdots \sim \mbox{Gamma}\left( 1 + \sum_{i : c_i = 2} x_i , 1 + n-m(\mathbf{c}) \right)$

where, as before, $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1 (so that $n - m(\mathbf{c})$ is the number of observations assigned to component 2), and $\sum_{c_i = 2}^{i} x_i$ is the sum of the observations assigned to component 2.

Hence, the code for this prompt might look something like:

```{r error=TRUE}
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
```

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Are the posterior means generated by the algorithm correct?**

Recall that the posterior means can be approximated by simple averages of the posterior samples obtained after burn in.  While there might a bit of Monte Carlo error in the answers reported, the values should be $E(\lambda∣x)≈3.05$ and $E(\omega∣x)≈0.40$. Note that these values are very close to those generated by the EM algorithm.

- 0点 No
- 1点 Yes

<br>

### 5th Peer

#### Asignment

Provide a Markov chain Monte Carlo algorithm to fit a zero-inflated Poisson distribution.

```{r error=TRUE}

```

Provide you estimates of the posterior means $\mbox{E}\left(\lambda \mid \mathbf{x} \right)$ and $\mbox{E}\left(w \mid \mathbf{x} \right)$.

```{r error=TRUE}

```

#### Marking

**Are the parameters initialized in a reasonable way?**

Correctly initializing the indicators $c_1, \ldots, c_n$ is key. In particular, observations such that $x_i \neq 0$ must have $c_i = 2$. As for observations for which $x_i = 0$, these can be randomly initialized to either component.

For the other parameters, it is reasonable to initialize $\lambda$ to the mean of the observations and the weight $\omega$ to a relatively small value (as we did in the EM algorithm).

Hence, the code for the initialization of the algorithm might look something like:

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=T, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto \begin{cases} w & x_i=0 \\ 0 & \mbox{otherwise} \end{cases}$

while

$\Pr(c_i = 2 \mid \cdots) \propto \begin{cases} (1-w) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} & x_i=0 \\ 1 & \mbox{otherwise} \end{cases}$

Hence, the code to sample the indicators might look something like this:

```{r error=TRUE}
# Full conditional for cc
for(i in 1:n){
  v = rep(0,2)
  if(x[i]==0){
    v[1] = log(w)
    v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
    v    = exp(v - max(v))/sum(exp(v - max(v)))
  }else{
    v[1] = 0
    v[2] = 1
  }
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the weight ww correct?**

This is a simple one, as its structure is common to all mixture models.  Recalling that the prior on $\omega$ is a uniform distribution on [0,1], we have

$\omega \mid \cdots \sim \mbox{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)$

where $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1.  The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the rate $\lambda$ correct?**

Because we use a exponential prior on $\lambda$, the full conditional posterior is a Gamma distribution,

$\lambda \mid \cdots \sim \mbox{Gamma}\left( 1 + \sum_{i : c_i = 2} x_i , 1 + n-m(\mathbf{c}) \right)$

where, as before, $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1 (so that $n - m(\mathbf{c})$ is the number of observations assigned to component 2), and $\sum_{c_i = 2}^{i} x_i$ is the sum of the observations assigned to component 2.

Hence, the code for this prompt might look something like:

```{r error=TRUE}
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
```

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Are the posterior means generated by the algorithm correct?**

Recall that the posterior means can be approximated by simple averages of the posterior samples obtained after burn in.  While there might a bit of Monte Carlo error in the answers reported, the values should be $E(\lambda∣x)≈3.05$ and $E(\omega∣x)≈0.40$. Note that these values are very close to those generated by the EM algorithm.

- 0点 No
- 1点 Yes

<br>

### 6th Peer

#### Asignment

Provide a Markov chain Monte Carlo algorithm to fit a zero-inflated Poisson distribution.

```{r error=TRUE}

```

Provide you estimates of the posterior means $\mbox{E}\left(\lambda \mid \mathbf{x} \right)$ and $\mbox{E}\left(w \mid \mathbf{x} \right)$.

```{r error=TRUE}

```

#### Marking

**Are the parameters initialized in a reasonable way?**

Correctly initializing the indicators $c_1, \ldots, c_n$ is key. In particular, observations such that $x_i \neq 0$ must have $c_i = 2$. As for observations for which $x_i = 0$, these can be randomly initialized to either component.

For the other parameters, it is reasonable to initialize $\lambda$ to the mean of the observations and the weight $\omega$ to a relatively small value (as we did in the EM algorithm).

Hence, the code for the initialization of the algorithm might look something like:

```{r error=TRUE}
n        = length(x)
cc       = rep(0, n)
cc[x==0] = sample(1:2, sum(x==0), replace=T, prob=c(1/2, 1/2))
cc[x!=0] = 2
lambda   = mean(x)
w        = 0.2
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the indicators $c_1, \ldots, c_n$ correct?**

The indicators $c_1, \ldots, c_n$ are conditionally independent from each other, and we have

$\Pr(c_i = 1 \mid \cdots) \propto \begin{cases} w & x_i=0 \\ 0 & \mbox{otherwise} \end{cases}$

while

$\Pr(c_i = 2 \mid \cdots) \propto \begin{cases} (1-w) \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} & x_i=0 \\ 1 & \mbox{otherwise} \end{cases}$

Hence, the code to sample the indicators might look something like this:

```{r error=TRUE}
# Full conditional for cc
for(i in 1:n){
  v = rep(0,2)
  if(x[i]==0){
    v[1] = log(w)
    v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
    v    = exp(v - max(v))/sum(exp(v - max(v)))
  }else{
    v[1] = 0
    v[2] = 1
  }
  cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
}
```

Please be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the weight ww correct?**

This is a simple one, as its structure is common to all mixture models.  Recalling that the prior on $\omega$ is a uniform distribution on [0,1], we have

$\omega \mid \cdots \sim \mbox{Beta}\left(m(\mathbf{c})+1, n-m(\mathbf{c})+1\right)$

where $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1.  The associated code might look something like this

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))
```

or, alternatively,

```{r error=TRUE}
# Full conditional for w
w = rbeta(1, 1+sum(cc==1), 1+sum(cc==2))
```

Please remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Is the full conditional for the rate $\lambda$ correct?**

Because we use a exponential prior on $\lambda$, the full conditional posterior is a Gamma distribution,

$\lambda \mid \cdots \sim \mbox{Gamma}\left( 1 + \sum_{i : c_i = 2} x_i , 1 + n-m(\mathbf{c}) \right)$

where, as before, $m(\mathbf{c})$ is the number of observations that $\mathbf{c}$ assigns to component 1 (so that $n - m(\mathbf{c})$ is the number of observations assigned to component 2), and $\sum_{c_i = 2}^{i} x_i$ is the sum of the observations assigned to component 2.

Hence, the code for this prompt might look something like:

```{r error=TRUE}
lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
```

- 0点 No
- <span style='color:green;'>1点 Yes</span>

**Are the posterior means generated by the algorithm correct?**

Recall that the posterior means can be approximated by simple averages of the posterior samples obtained after burn in.  While there might a bit of Monte Carlo error in the answers reported, the values should be $E(\lambda∣x)≈3.05$ and $E(\omega∣x)≈0.40$. Note that these values are very close to those generated by the EM algorithm.

- 0点 No
- 1点 Yes

<br><br>

## ディスカッション

<br><br>

# Appendix

## Blooper

## Documenting File Creation 

It's useful to record some information about how your file was created.

- File creation date: 2021-05-24
- File latest updated date: `r today('Asia/Tokyo')`
- `r R.version.string`
- [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
- File version: 1.0.0
- Author Profile: [®γσ, Eng Lian Hu](https://github.com/scibrokes/owner)
- GitHub: [Source Code](https://github.com/englianhu/coursera-bayesian-statistics-mixture-models)
- Additional session information:

```{r info, warning=FALSE, error=TRUE, results='asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('magrittr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))
suppressMessages(require('knitr', quietly = TRUE))
suppressMessages(require('kableExtra', quietly = TRUE))

sys1 <- devtools::session_info()$platform %>%
  unlist %>%
  data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL

sys2 <- data.frame(Sys.info()) %>%
  dplyr::mutate(Category = rownames(.)) %>%
  .[2:1]
names(sys2)[2] <- c('Sys.info')
rownames(sys2) <- NULL

if (nrow(sys1) == 9 & nrow(sys2) == 8) {
  sys2 %<>% rbind(., data.frame(
  Category = 'Current time', 
  Sys.info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
} else {
  sys1 %<>% rbind(., data.frame(
  Category = 'Current time', 
  session_info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
}

sys <- cbind(sys1, sys2) %>%
  kbl(caption = 'Additional session information:') %>%
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>%
  row_spec(0, background = 'DimGrey', color = 'yellow') %>%
  column_spec(1, background = 'CornflowerBlue', color = 'red') %>%
  column_spec(2, background = 'grey', color = 'black') %>%
  column_spec(3, background = 'CornflowerBlue', color = 'blue') %>%
  column_spec(4, background = 'grey', color = 'white') %>%
  row_spec(9, bold = T, color = 'yellow', background = '#D7261E')

rm(sys1, sys2)
sys
```

## Reference

<br>

---

<br>
