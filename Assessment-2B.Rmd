---
title: "<img src='figure/coursera.jpg' width='37'> <img src='figure/ucsc.png' width='240'>"
subtitle: "<span style='color:white; background-color:#4E79A7;'>Bayesian Statistics: Mixture Models</span> (Assessment Week2 B Codes)"
author: "[®γσ, Lian Hu](https://englianhu.github.io/) <img src='figure/quantitative trader 1.jpg' width='12'> <img src='figure/ENG.jpg' width='24'> ®"
date: "`r lubridate::today('Asia/Tokyo')`"
output:
  html_document: 
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    css: CSSBackgrounds.css
---

<br>
<span style='color:green'>**Theme Song**</span>
<br>

<audio src="music/California-Dreaming-Chorus.mp3" controls></audio>
<br>

------

<br>

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

*Source : [Width of R code chunk output in RMarkdown files knitr-ed to html](https://stackoverflow.com/a/36846864/3806250)*

```{css class.source = 'bg-success', class.output = 'bg-primary'}
/* https://stackoverflow.com/a/66029010/3806250 */
h1 { color: #002C54; }
h2 { color: #2F496E; }
h3 { color: #375E97; }

/* https://bookdown.org/yihui/rmarkdown-cookbook/chunk-styling.html */
.gradient1 {
  background: linear-gradient(155deg, #F9BA32 0%, #FFEB94 100%);
}

.gradient2 {
  color: #FFD64D;
  background: linear-gradient(155deg, #002C54 0%, #4CB5F5 100%);
  /*background-image: linear-gradient(to right,golden,golden, yellow, yellow);*/
  /*-webkit-background-clip: text;*/
  /*display: inline-block;*/
  /*padding: 14px;*/
  /*-webkit-text-fill-color: transparent;*/
  /*font-family: 'Stay Out Regular';*/
}

.shine {
	background: #FFD64D -webkit-gradient(linear, left top, right top, from(#222), to(#222), color-stop(0.5, yellow)) 0 0 no-repeat;
	-webkit-background-size: 150px;
	color: $text-color;
	-webkit-background-clip: text;
	-webkit-animation-name: shine;
	-webkit-animation-duration: $duration;
	-webkit-animation-iteration-count: infinite;
	text-shadow: 0 0px 0px golden-rod;
}
```

```{r global_options, class.source = 'bg-success', class.output = 'bg-primary'}
## https://stackoverflow.com/a/36846793/3806250
options(width = 999)
knitr::opts_chunk$set(class.source = 'gradient1', class.output = 'gradient2', class.error = 'bg-danger')
```

<br><br>

# 受講生によるテストの実施：The EM algorithm for zero-inflated mixtures

**課題を受ける準備はできていますか？**

以下に提出のための指示が表示されます。

<br><br>

## 説明

A biologist is interest in characterizing the number of eggs laid by a particular bird species.  To do this, they sample n=300n=300 nests on a site in Southern California.  The observations are contained in the attached file [`nestsize.csv`](https://github.com/englianhu/Coursera-Bayesian-Statistics-Mixture-Models/blob/main/data/nestsize.csv):

<br><br>

### Review criteria




<br><br>

## 自分の提出物

## ピアレビューディスカッション

### ルーブリック

#### Setup

```{r setup, warning = FALSE, message = FALSE}
if(!suppressPackageStartupMessages(require('BBmisc'))) {
  install.packages('BBmisc', dependencies = TRUE, INSTALL_opts = '--no-lock')
}
suppressPackageStartupMessages(require('BBmisc'))
# suppressPackageStartupMessages(require('rmsfuns'))

pkgs <- c('devtools', 'knitr', 'kableExtra', 'tidyr', 
          'readr', 'lubridate', 'data.table', 'reprex', 
          'timetk', 'plyr', 'dplyr', 'stringr', 'magrittr', 
          'tdplyr', 'tidyverse', 'formattable', 
          'echarts4r', 'paletteer')

suppressAll(lib(pkgs))
# load_pkg(pkgs)

## Set the timezone but not change the datetime
Sys.setenv(TZ = 'Asia/Tokyo')
## options(knitr.table.format = 'html') will set all kableExtra tables to be 'html', otherwise need to set the parameter on every single table.
options(warn = -1, knitr.table.format = 'html')#, digits.secs = 6)

## https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-abnor-in-rmd-but-not-in-r-script
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

rm(pkgs)
```

```{r error = TRUE}
dat <- fread('data/nestsize.csv') %>% as.matrix
head(dat)
```

<span style='color:green;'>*Source : `r paste(dim(dat), collapse = ' x ')`*</span>

#### Assignment

```{r error = TRUE}

```

Reference from tutorial : [Sample Code for EM Example 2](https://github.com/englianhu/Coursera-Bayesian-Statistics-Mixture-Models/blob/main/Sample-Code-for%20EM-EX2.R)

<br><br>

### ルーブリック

**Are the initial values reasonable?**

In this problem there are only two unknown parameters:  the weight of the mixture ($\omega$), and the mean of the Poisson component ($\lambda$).  As long as the parameters are in the right range (i.e., $0 < \omega < 1$ and $\lambda > 0$), you should give credit for this prompt.

An example of a default choice that would likely work well in most cases would be something like: 

```{r error = TRUE}
w      = 0.5
lambda = mean(x)
```

- 0点 No

- 1点 Yes

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?**

Note that two cases need to be considered. If the observation is a zero, then in can come from either the point mass or the Poisson distribution. On the other hand, if the observation is different from zero, it can only come from the Poisson component. Hence:

$$v_{i,1}\propto \left\{\begin{matrix}
\omega & x_i=0 \\
0 & otherwise \\
\end{matrix}\right.$$

and

$$v_{i,2}\propto \left\{\begin{matrix}
(1-\omega)\frac{e^{-\lambda}\lambda^{x_i} }{x!} & x_i=0 \\
1 & otherwise \\
\end{matrix}\right.$$

Hence, the code for the E step could look something like

```{r error = TRUE}
## E step
v = array(0, dim=c(n,2))
for(i in 1:n){
  if(x[i]==0){
    v[i,1] = w    
    v[i,2] = (1-w)*dpois(x[i], lambda)  
    v[i,]  = v[i,]/sum(v[i,])
  }else{
    v[i,1] = 0
    v[i,2] = 1
  }
}
```

However, be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No

- 1点 Yes

**Are the formulas for the maximum of the $Q$ functions correct (M step)?**

Again, when computing the $Q$ you need to consider two cases. In particular, recall that the $Q$ function can be written as a sum over the observations:

$$Q\left(w, \lambda \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)} \right) = \\ \sum_{i=1}^{n} Q_i\left(w, \lambda \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)} , x_i \right)$$

where

$$Q_i\left(w, \lambda \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)} , x_i \right) = \\ \begin{cases} v_{i,1} \log w + v_{i,2} \left[ \log (1-w)  -\lambda + x_i \log \lambda - \log x_i! \right] & x_i=0 \\ v_{i,2} \left[ \log (1-w)  -\lambda + x_i \log \lambda - \log x! \right] & \text{otherwise} \end{cases} $$
Computing the derivatives and setting them to zero yields

$\hat{\omega}^(t+1) = \frac{1}{n}\sum_{i=1}^{n}v_{i,1}$ and $\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} x_{i}}{\sum_{i=1}^{n} v_{i,2}}$.

Hence, the code for this section might look something like:

```{r error = TRUE}
w = mean(v[,1])
lambda = sum(x)/sum(v[,2])
```

- 0点 No
- 1点 Yes

**Is the converge check correct?**

The only change required in the sample code has to do with the calculation of the Q function. As discussed in the previous prompt, this reduces to

$$Q\left(w, \lambda \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)} \right) = \\ \sum_{i=1}^{n} Q_i\left(w, \lambda \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)} , x_i \right)$$

where

$$Q_i\left(w, \lambda \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)} , x_i \right) = \\ \begin{cases} v_{i,1} \log w + v_{i,2} \left[ \log (1-w)  -\lambda + x_i \log \lambda - \log x_i! \right] & x_i=0 \\ v_{i,2} \left[ \log (1-w)  -\lambda + x_i \log \lambda - \log x! \right] & \text{otherwise} \end{cases} $$

Hence, the code for this prompt might look something like:

```{r error = TRUE}
##Check convergence
QQn = 0
for(i in 1:n){
  if(x[i]==0){
    QQn = QQn + v[i,1]*log(w) + v[i,2]*(log(1-w) + dpois(x[i], lambda, log=TRUE))
  }else{
    QQn = QQn + v[i,2]*(log(1-w) + dpois(x[i], lambda, log=TRUE))
  }
}
if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
}
QQ = QQn
```

- 0点 No

- 1点 Yes

Provide your maximum likelihood estimates $\hat{\omega}$ and $\hat{\lambda}$.

$w_hat = 0.3192758$
$lambda_hat = 2.704756$

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{\lambda} = 3.07$ and $\hat{w} = 0.40$.

- 0点 No

- 1点 Yes

<br><br>

## レビュー

### 1st Peer

```{r error = TRUE}
#### Example of an EM algorithm for fitting a location mixture of 2 Gaussian components
#### The algorithm is tested using simulated data

## Clear the environment and load required libraries
rm(list=ls())
#set.seed(81196)    # So that results are reproducible (same simulated data every time)
set.seed(12345)

x = read.csv("data/nestsize.csv",header=FALSE)$V1
KK         = 2

## Run the actual EM algorithm
## Initialize the parameters
w     = 0.5                       #Assign equal weight to each component to start with
lambda    = mean(x)   #Random cluster centers randomly spread over the support of the data
n = length(x)

# Plot the initial guess for the density
xx = seq(0,10,length=11)
ddeg = function(x,log=FALSE) {
  p = pmin(0.99999,pmax(0.00001,as.integer(x==0)))
  if(log==FALSE) {
    p
  } else {
    .Primitive("log")(pmax(1.0e-40,p)) 
  }
}
yy = w*ddeg(xx) + (1-w)*dpois(xx, lambda)
par(mfrow=c(1,1),mar=c(4,4,4,4))
plot(xx, yy, type="l", ylim=c(0, max(yy)), xlab="x", ylab="Initial density")
points(jitter(x), jitter(rep(0,n)),col="red")

s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-8)

##Checking convergence of the algorithm
while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v.logs = array(0, dim=c(n,KK))
  v.logs[,1] = log(w) + log(ddeg(x))    #Compute the log of the weights
  v.logs[,2] = log(1-w) + log(dpois(x, lambda))  #Compute the log of the weights
  for(i in 1:n){
    #v[i,] = exp(v.logs[i,])
    v[i,] = exp(v.logs[i,] - max(v.logs[i,]))/sum(exp(v.logs[i,] - max(v.logs[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step
  # Weights
  #w = max(1.0e-10,sum(v[,1])/(sum(v[,1])+sum(v[,2])))
  w = mean(v[,1])#sum(v[,1])/(sum(v[,1])+sum(v[,2]))
  mu = 0
  for(i in 1:n){
    mu    = mu + v[i,2]*x[i]
  }
  mu=mu/sum(v[,2])
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    logp1 = ddeg(x[i],log=TRUE)
    first = v[i,1]*(log(w) + logp1)
    second = v[i,2]*(log(1-w) + dpois(x, lambda,log=TRUE))
    QQn = QQn + first + second
    #cat("v[",i,",1]",v[i,1],"v[",i,",2]",v[i,2],"w",w,"logp1",logp1,"QQn",QQn,"first",first,"second",second,"\n")
  }
  epsilon_cond = abs(QQn-QQ)/abs(QQn)
  cat("QQn",QQn,"QQ",QQ,"epsilon_cond",epsilon_cond,"mu",mu,"w",w,"\n")
  if(epsilon_cond<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  
  s = s + 1
  print(paste(s, QQn))
  
  #Plot current estimate over data
  layout(matrix(c(1,2,3),3,1), widths=c(1,1,1), heights=c(1,1,3))
  par(mar=c(3.1,4.1,0.5,0.5))
  plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

  par(mar=c(5,4,1.5,0.5))
  xx = seq(0,10,length=11)
  yy = w*ddeg(xx) + (1-w)*dpois(xx, mu)
  plot(xx, yy, type="l", ylim=c(0,1), main=paste("s =",s,"   Q =", round(QQ.out[s],4)," w=",w," mu=",mu), lwd=2, col="red", lty=2, xlab="x", ylab="Density")
  #lines(xx.true, yy.true, lwd=2)
  points(x, rep(0,n), col="red")
  legend(6,0.22,c("Truth","Estimate"),col=c("black","red"), lty=c(1,2))
}

#Plot final estimate over data
layout(matrix(c(1,2,3),3,1), widths=c(1,1,1), heights=c(1,1,3))
plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

par(mar=c(5,4,1.5,0.5))
xx = seq(0,10,length=11)
yy = w*ddeg(xx) + (1-w)*dpois(xx, mu)
plot(xx, yy, type="l", ylim=c(0,1), main=paste("s =",s,"   Q =", round(QQ.out[s],4)," w=",w," mu=",mu), lwd=2, col="red", lty=2, xlab="x", ylab="Density")
points(x, rep(0,n), col="red")
legend(6,0.22,c("Truth","Estimate"),col=c("black","red"), lty=c(1,2), bty="n")

paste('w =', w); paste('mu =', mu)
paste('QQ =', QQ); paste('QQn =', QQn)
```

<br><br>

### 2nd Peer

Provide the EM algorithm to fit a zero-inflated Poisson distribution.

```{r error = TRUE}
w     = 1/2  #Assign equal weight to each component to start with
maxv = 1 - 1e-9
mu = c(0, 4) # mean of uniform and poisson dist respectively
KK = 2

## E step
v = array(0, dim=c(n,KK))
v[,1] = log(w) + dunif(x , max = maxv, log = T)
v[,2] = log(1-w) + dpois(x, mu[2], log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}

w = mean(v[,1])
mu = rep(0, KK)
for(k in 2:KK) { # can ignore calculation of mu[1]
  for(i in 1:n){
    mu[k]    = mu[k] + v[i,k]*x[i]
  }
  mu[k] = mu[k]/sum(v[,k])
}

##Check convergence
QQn = 0
for(i in 1:n){
  comp1 = log(w) + dunif(x[i] , max = maxv, log = T)
  if (comp1 < -1e9) {comp1 = 0} # turn negative inf to zero
  QQn = QQn + v[i,1]*(comp1) + v[i,2]*(log(1-w) + dpois(x[i], mu[2], log=TRUE))
}
if(abs(QQn-QQ)/abs(QQn)<epsilon){sw=TRUE}
QQ = QQn

paste('w =', w); paste('mu =', mu)
paste('QQ =', QQ); paste('QQn =', QQn)
```

Provide your maximum likelihood estimates $\hat{\omega}$ and $\hat{\lambda}$.

w = 0.3986 or 0.40
m = 3.065 or 3.07

<br><br>

### 3rd Peer

Provide the EM algorithm to fit a zero-inflated Poisson distribution.

```{r}
w = 0.5

v = array(0, dim=c(n,2))

for(i in 1:n){
  if(x[i]==0){
    v[i,1] = w
    v[i,2] = (1-w)*dpois(x[i], lambda)
    v[i,]  = v[i,]/sum(v[i,])
    
  }else{
    v[i,1] = 0
    v[i,2] = 1
  }
}

w = mean(v[,1])
lambda = sum(x)/sum(v[,2])
QQn = 0

for(i in 1:n){
  if(x[i]==0){
    QQn = QQn + v[i,1]*log(w) + v[i,2]*(log(1-w) + dpois(x[i], lambda, log=TRUE))
    
  }else{
    QQn = QQn + v[i,2]*(log(1-w) + dpois(x[i], lambda, log=TRUE))
  }
}

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
}

paste('w =', w); paste('lambda =', lambda)
paste('QQ =', QQ); paste('QQn =', QQn)
```

Provide your maximum likelihood estimates $\hat{\omega}$ and $\hat{\lambda}$.

MLE for w = 0.3986 ~ 0.4
MLE for lambda = 3.0651 ~ 3.07

<br><br>

## ディスカッション

<br><br>

# Appendix

## Blooper

## Documenting File Creation 

It's useful to record some information about how your file was created.

- File creation date: 2021-05-12
- File latest updated date: `r today('Asia/Tokyo')`
- `r R.version.string`
- [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
- File version: 1.0.0
- Author Profile: [®γσ, Eng Lian Hu](https://github.com/scibrokes/owner)
- GitHub: [Source Code](https://github.com/englianhu/coursera-bayesian-statistics-mixture-models)
- Additional session information:

```{r info, warning = FALSE, results = 'asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('magrittr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))
suppressMessages(require('knitr', quietly = TRUE))
suppressMessages(require('kableExtra', quietly = TRUE))

sys1 <- devtools::session_info()$platform %>% 
  unlist %>% data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL

sys2 <- data.frame(Sys.info()) %>% 
  dplyr::mutate(Category = rownames(.)) %>% .[2:1]
names(sys2)[2] <- c('Sys.info')
rownames(sys2) <- NULL

if (nrow(sys1) == 9 & nrow(sys2) == 8) {
  sys2 %<>% rbind(., data.frame(
  Category = 'Current time', 
  Sys.info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
} else {
  sys1 %<>% rbind(., data.frame(
  Category = 'Current time', 
  session_info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
}

sys <- cbind(sys1, sys2) %>% 
  kbl(caption = 'Additional session information:') %>% 
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>% 
  row_spec(0, background = 'DimGrey', color = 'yellow') %>% 
  column_spec(1, background = 'CornflowerBlue', color = 'red') %>% 
  column_spec(2, background = 'grey', color = 'black') %>% 
  column_spec(3, background = 'CornflowerBlue', color = 'blue') %>% 
  column_spec(4, background = 'grey', color = 'white') %>% 
  row_spec(9, bold = T, color = 'yellow', background = '#D7261E')

rm(sys1, sys2)
sys
```

## Reference

- [Bayesian Statistics: Mixture Models (Assessment Week2 A Codes)](https://rpubs.com/englianhu/768289)

<br>

---

<br>
