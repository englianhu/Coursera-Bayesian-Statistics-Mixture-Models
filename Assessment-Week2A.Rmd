---
title: "<img src='figure/coursera.jpg' width='37'> <img src='figure/ucsc.png' width='240'>"
subtitle: "<span style='color:white; background-color:#4E79A7;'>Bayesian Statistics: Mixture Models</span> (Assessment Week2 A Codes)"
author: "[®γσ, Lian Hu](https://englianhu.github.io/) <img src='figure/quantitative trader 1.jpg' width='12'> <img src='figure/ENG.jpg' width='24'> ®"
date: "`r lubridate::today('Asia/Tokyo')`"
output:
  html_document: 
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    css: CSSBackgrounds.css
---

<br>
<span style='color:green'>**Theme Song**</span>
<br>

<audio src="music/California-Dreaming-Chorus.mp3" controls></audio>
<br>

------

# Setting

## SCSS Setup

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
.table-hover > tbody > tr:hover { 
  background-color: #8D918D;
}
</style>

```{r class.source = 'bg-success', class.output = 'bg-primary', message = FALSE, warning = FALSE}
# install.packages("remotes")
require('BBmisc')
#remotes::install_github("rstudio/sass")
lib('sass')
```

```{scss class.source = 'bg-success', class.output = 'bg-primary'}
/* https://stackoverflow.com/a/66029010/3806250 */
h1 { color: #002C54; }
h2 { color: #2F496E; }
h3 { color: #375E97; }

/* ----------------------------------------------------------------- */
/* https://gist.github.com/himynameisdave/c7a7ed14500d29e58149#file-broken-gradient-animation-less */
.hover01 {
  /* color: #FFD64D; */
  background: linear-gradient(155deg, #EDAE01 0%, #FFEB94 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #EDAE01 20%, #FFEB94 80%);
    }
  }

.hover02 {
  color: #FFD64D;
  background: linear-gradient(155deg, #002C54 0%, #4CB5F5 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #002C54 20%, #4CB5F5 80%);
    }
  }

.hover03 {
  color: #FFD64D;
  background: linear-gradient(155deg, #A10115 0%, #FF3C5C 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #A10115 20%, #FF3C5C 80%);
    }
  }
```

```{r global_options, class.source = 'hover01', class.output = 'hover02'}
## https://stackoverflow.com/a/36846793/3806250
options(width = 999)
knitr::opts_chunk$set(class.source = 'hover01', class.output = 'hover02', class.error = 'hover03')
```

<br><br>

## Setup

```{r setup, warning=FALSE, message=FALSE}
if(!suppressPackageStartupMessages(require('BBmisc'))) {
  install.packages('BBmisc', dependencies = TRUE, INSTALL_opts = '--no-lock')
}
suppressPackageStartupMessages(require('BBmisc'))
# suppressPackageStartupMessages(require('rmsfuns'))

pkgs <- c('devtools', 'knitr', 'kableExtra', 'tidyr', 
          'readr', 'lubridate', 'data.table', 'reprex', 
          'timetk', 'plyr', 'dplyr', 'stringr', 'magrittr', 
          'tdplyr', 'tidyverse', 'formattable', 
          'echarts4r', 'paletteer')

suppressAll(lib(pkgs))
# load_pkg(pkgs)

## Set the timezone but not change the datetime
Sys.setenv(TZ = 'Asia/Tokyo')
## options(knitr.table.format = 'html') will set all kableExtra tables to be 'html', otherwise need to set the parameter on every single table.
options(warn = -1, knitr.table.format = 'html')#, digits.secs = 6)

## https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-abnor-in-rmd-but-not-in-r-script
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
#, 
                      #cache = TRUE, cache.lazy = FALSE)

rm(pkgs)
```

<br><br>

# 受講生によるテスト：The EM algorithm for Mixture Models

5月17日 15:59 JST までに提出

**課題をすぐに提出してください**

課題の提出期限は、5月17日 15:59 JSTですが、可能であれば1日か2日早く提出してください。

早い段階で提出すると、他の受講生のレビューを時間内に得る可能性が高くなります。

<br><br>

## 説明

Data on the lifetime (in years) of fuses produced by the ACME Corporation is available in the file `fuses.csv`:

In order to characterize the distribution of the lifetimes, it seems reasonable to fit to the data a two-component mixture of the form:

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

The first component, which corresponds to an exponential distribution with rate $\lambda$, is used to model low-quality components with a very short lifetime.  The second component, which corresponds to a log-Gaussian distribution, is used to model normal, properly-functioning components.

You are asked to modify the implementation of the EM algorithm contained in the Reading "Sample code for EM example 1" so that you can fit this two-component mixture distributions instead. You then should run your algorithm with the data contained in `fuses.csv` and report the values of the estimates for $w$, $\lambda$, $\mu$ and $\tau$ that you obtained, rounded to two decimal places.

<br><br>

### Review criteria

The code you generate should follow the same structure as "Sample code for EM example 1". Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect the fact that (1) you provided a reasonable initial point for you algorithm, (2) the observation-specific weights $v_{i,k}$ are computed correctly (E step), (3) the formulas for the maximum of the QQ functions are correct (M step), (4) the converge check is correct, and (5) the numerical values that you obtain are correct.

<br><br>

## 自分の提出物

### Assignment

```{r error=TRUE}
#read the data from the file
dat = read.csv(file = "data/fuses.csv", header = FALSE)
fuses = dat$V1
logfuses = log(fuses)

## Run the actual EM algorithm
## Initialize the parameters

KK         = 2                               # number of components
n = length(fuses)                         # number of samples
v = array(0, dim=c(n,KK))
v[,1] = 0.5                    #Assign half weight to first component
v[,2] = 1-v[,1]                #Assign all of the remaining weights to the second component
mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
lambda = 1.0/mean1            #parameter for the first component
mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
tau = sqrt(tausquared)
w = mean(v[,1])
print(paste(lambda, mu, tausquared, tau, w))


s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-5)

##Checking convergence of the algorithm
while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v[,1] = log(w) + dexp(fuses, lambda,log=TRUE)    #Compute the log of the weights
  v[,2] = log(1-w) + dnorm(logfuses, mu, tau, log=TRUE)  #Compute the log of the weights
  for(i in 1:n){
    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step
  # Weights
  w = mean(v[,1])
  ## parameters
  mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
  lambda = 1.0/mean1            #parameter for the first component
  mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
  tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
  tau = sqrt(tausquared)
  w = mean(v[,1])
  print(paste(s, lambda, mu, tausquared, tau, w))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    QQn = QQn + v[i,1]*(log(w) + dexp(fuses[i], lambda, log=TRUE)) +
      v[i,2]*(log(1-w) + dnorm(logfuses[i], mu, tau, log=TRUE))
  }
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
}
```

```{r error=TRUE}
#Plot final estimate over data
layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))
par(mar=c(3.1,4.1,0.5,0.5))

#plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

## https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html
## https://echarts.apache.org/en/theme-builder.html
Qplot <- tibble(Index = 1:length(QQ.out[1:s]), Value = QQ.out[1:s])
Qplot |>
  e_charts(x = Index) |>
  e_theme('dark-mushroom') |>
  e_line(Value, stack = 'grp2') |>
  e_datazoom(
    type = "slider", 
    toolbox = FALSE,
    bottom = -5) |>
  e_tooltip() |>
  e_title('Exopential Regression', 'Plot', sublink = 'https://echarts4r.john-coene.com/reference/e_bar.html') |>
  e_x_axis(Index, axisPointer = list(show = TRUE)) |>
  e_legend(
    orient = 'vertical', 
    type = c('scroll'), 
    right = 80)
```

Reference from tutorial : [Sample Code for EM Example 1](https://github.com/englianhu/Coursera-Bayesian-Statistics-Mixture-Models/blob/main/Week2%20Sample%20Code%20for%20EM%20EX1.R)

<br><br>

### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error=TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0点 No
- 1点 Some are, but not all
- <span style='color:Green;'>2点 Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error=TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error=TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- 1点 Some are correct, but not all of them
- <span style='color:Green;'>3点 Yes</span>

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error=TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- 0点 No
- <span style='color:Green;'>2点 Yes</span>

<br><br>

## ピアレビュー

### 1st Peer

#### Asignment

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r error=TRUE}
dat = read.csv(file = "data/fuses.csv", header = FALSE)
x <- dat$V1
w     = 1/2        #Assign equal weight to each component to start with
sigma = sd(log(x)) # take sigma to be tau
mu <- c(.02, 2)
KK <- 2
QQ = -Inf
epsilon <- 10^(-8)

## E step
v = array(0, dim=c(n,KK))
v[,1] = log(w) + dexp(x, mu[1], log=T)
v[,2] = log(1-w) + dlnorm(x, mu[2], sigma, log=T)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
}

## M step
# Weights
w = mean(v[,1])
mu[1] = sum(v[,1]) / sum(v[,1] * x)

mu[2] = sum(v[,2] * log(x)) / sum(v[,2])


# Standard deviations
sigma = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))

##Check convergence
QQn = 0
for(i in 1:n){
  comp1 = log(w) + dexp(x[i], mu[1], log=T);
  comp2 = log(1-w) + dlnorm(x[i], mu[2], sigma, log=T)
  QQn = QQn + v[i,1]*(comp1) + v[i,2]*(comp2)
}
#print(QQ)
if(abs(QQn-QQ)/abs(QQn)<epsilon){sw=TRUE}
QQ = QQn

QQ
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

$w = 0.09, lambda = 3.05, mu = 0.79, tau = 0.38$

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error=TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0点 No
- 1点 Some are, but not all
- <span style='color:Green;'>2点 Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error=TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error=TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- 1点 Some are correct, but not all of them
- <span style='color:Green;'>3点 Yes</span>

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error=TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- 0点 No
- <span style='color:Green;'>2点 Yes</span>

<br>

### 2nd Peer

#### Asignment

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r error=TRUE}
#read the data from the file
dat = read.csv(file = "data/fuses.csv", header = FALSE)
fuses = dat$V1
logfuses = log(fuses)

## Run the actual EM algorithm
## Initialize the parameters

KK         = 2                               # number of components
n = length(fuses)                         # number of samples
v = array(0, dim=c(n,KK))
v[,1] = 0.5                    #Assign half weight to first component
v[,2] = 1-v[,1]                #Assign all of the remaining weights to the second component
mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
lambda = 1.0/mean1            #parameter for the first component
mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
tau = sqrt(tausquared)
w = mean(v[,1])
print(paste(lambda, mu, tausquared, tau, w))


s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-5)

##Checking convergence of the algorithm
while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v[,1] = log(w) + dexp(fuses, lambda,log=TRUE)    #Compute the log of the weights
  v[,2] = log(1-w) + dnorm(logfuses, mu, tau, log=TRUE)  #Compute the log of the weights
  for(i in 1:n){
    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step
  # Weights
  w = mean(v[,1])
  ## parameters
  mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
  lambda = 1.0/mean1            #parameter for the first component
  mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
  tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
  tau = sqrt(tausquared)
  w = mean(v[,1])
  print(paste(s, lambda, mu, tausquared, tau, w))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    QQn = QQn + v[i,1]*(log(w) + dexp(fuses[i], lambda, log=TRUE)) +
      v[i,2]*(log(1-w) + dnorm(logfuses[i], mu, tau, log=TRUE))
  }
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
}

#Plot final estimate over data
layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))
par(mar=c(3.1,4.1,0.5,0.5))
plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

$w = 0.09, lambda = 3.05, mu = 0.79, tau = 0.38$

#### Marking

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $w = 0.09$, $lambda = 3.05$, $mu = 0.79$, $tau = 0.38$.

- 0点 No
- <span style='color:Green;'>2点 Yes</span>

<br>

### 3rd Peer

#### Asignment

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r error=TRUE}
#### Example of an EM algorithm for fitting a location mixture of 2 Gaussian components
#### The algorithm is tested using simulated data

## Clear the environment and load required libraries
rm(list=ls())
#set.seed(81196)    # So that results are reproducible (same simulated data every time)
set.seed(12345)

x = read.csv("data/nestsize.csv", header=FALSE)$V1
KK         = 2

## Run the actual EM algorithm
## Initialize the parameters
w     = 0.5                       #Assign equal weight to each component to start with
lambda    = mean(x)   #Random cluster centers randomly spread over the support of the data
n = length(x)

# Plot the initial guess for the density
xx = seq(0,10,length=11)
ddeg = function(x,log=FALSE) {
  p = pmin(0.99999,pmax(0.00001,as.integer(x==0)))
  if(log==FALSE) {
    p
  } else {
    .Primitive("log")(pmax(1.0e-40,p)) 
  }
}
yy = w*ddeg(xx) + (1-w)*dpois(xx, lambda)
par(mfrow=c(1,1),mar=c(4,4,4,4))
plot(xx, yy, type="l", ylim=c(0, max(yy)), xlab="x", ylab="Initial density")
points(jitter(x), jitter(rep(0,n)),col="red")

s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-8)

##Checking convergence of the algorithm
while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v.logs = array(0, dim=c(n,KK))
  v.logs[,1] = log(w) + log(ddeg(x))    #Compute the log of the weights
  v.logs[,2] = log(1-w) + log(dpois(x, lambda))  #Compute the log of the weights
  for(i in 1:n){
    #v[i,] = exp(v.logs[i,])
    v[i,] = exp(v.logs[i,] - max(v.logs[i,]))/sum(exp(v.logs[i,] - max(v.logs[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step
  # Weights
  #w = max(1.0e-10,sum(v[,1])/(sum(v[,1])+sum(v[,2])))
  w = mean(v[,1])#sum(v[,1])/(sum(v[,1])+sum(v[,2]))
  mu = 0
  for(i in 1:n){
    mu    = mu + v[i,2]*x[i]
  }
  mu=mu/sum(v[,2])
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    logp1 = ddeg(x[i],log=TRUE)
    first = v[i,1]*(log(w) + logp1)
    second = v[i,2]*(log(1-w) + dpois(x, lambda,log=TRUE))
    QQn = QQn + first + second
    #cat("v[",i,",1]",v[i,1],"v[",i,",2]",v[i,2],"w",w,"logp1",logp1,"QQn",QQn,"first",first,"second",second,"\n")
  }
  epsilon_cond = abs(QQn-QQ)/abs(QQn)
  cat("QQn",QQn,"QQ",QQ,"epsilon_cond",epsilon_cond,"mu",mu,"w",w,"\n")
  if(epsilon_cond<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  
  s = s + 1
  print(paste(s, QQn))
  
  #Plot current estimate over data
  layout(matrix(c(1,2,3),3,1), widths=c(1,1,1), heights=c(1,1,3))
  par(mar=c(3.1,4.1,0.5,0.5))
  plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

  par(mar=c(5,4,1.5,0.5))
  xx = seq(0,10,length=11)
  yy = w*ddeg(xx) + (1-w)*dpois(xx, mu)
  plot(xx, yy, type="l", ylim=c(0,1), main=paste("s =",s,"   Q =", round(QQ.out[s],4)," w=",w," mu=",mu), lwd=2, col="red", lty=2, xlab="x", ylab="Density")
  #lines(xx.true, yy.true, lwd=2)
  points(x, rep(0,n), col="red")
  legend(6,0.22,c("Truth","Estimate"),col=c("black","red"), lty=c(1,2))
}

#Plot final estimate over data
layout(matrix(c(1,2,3),3,1), widths=c(1,1,1), heights=c(1,1,3))
plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

par(mar=c(5,4,1.5,0.5))
xx = seq(0,10,length=11)
yy = w*ddeg(xx) + (1-w)*dpois(xx, mu)
plot(xx, yy, type="l", ylim=c(0,1), main=paste("s =",s,"   Q =", round(QQ.out[s],4)," w=",w," mu=",mu), lwd=2, col="red", lty=2, xlab="x", ylab="Density")
points(x, rep(0,n), col="red")
legend(6,0.22,c("Truth","Estimate"),col=c("black","red"), lty=c(1,2), bty="n")
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

$w = 0.09, lambda = 3.05, mu = 0.79, tau = 0.38$

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error=TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0点 No
- 1点 Some are, but not all
- <span style='color:Green;'>2点 Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error=TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error=TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- 1点 Some are correct, but not all of them
- <span style='color:Green;'>3点 Yes</span>

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error=TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- 0点 No
- <span style='color:Green;'>2点 Yes</span>

<br>

### 4th Peer

#### Asignment

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r error=TRUE}
## Read fuses.csv
df = read.csv("data/fuses.csv", header = FALSE)
x  = df$V1
n  = nrow(df)

## Initialize the parameters
w      = 1/2                   #Assign equal weight to each component to start with
mu     = mean(log(x))               #Random cluster centers randomly spread over the support of the data
tau    = sd(log(x))                   #Initial standard deviation
lambda = 1/mu   
```

```{r error=TRUE}
  ## E step
  v     = array(0, dim=c(n,KK))
  v[,1] = log(w) + dexp(x, lambda, log=TRUE)    #Compute the log of the weights
  v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)  #Compute the log of the weights
  for(i in 1:n){
    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
  }

  ## M step
  # Weights
  w      = mean(v[,1])
  lambda = 0 
  denom  = 0
  for(i in 1:n){
    denom = denom + v[i,1]*x[i]
  }
  lambda  = sum(v[,1])/denom

  for(i in 1:n){
    mu    = mu + v[i,2]*log(x[i])
  }
  mu = mu/sum(v[,2])

  # Standard deviations
  tau = 0
  for(i in 1:n){
    tau = tau + v[i,2]*(log(x[i]) - mu)^2
  }
  tau = sqrt(tau/sum(v[,2]))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) +
      v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }
  
QQn
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

```{r error=TRUE}
print(paste('w =', round(w,2))) #0.09
print(paste('lambda =', round(lambda,2))) #3.05
print(paste('mu =', round(mu,2))) #0.78
print(paste('tau =', round(tau,2))) #0.38
```

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error=TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0点 No
- 1点 Some are, but not all
- <span style='color:Green;'>2点 Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error=TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error=TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- <span style='color:Green;'>1点 Some are correct, but not all of them</span>
- 3点 Yes

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error=TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- <span style='color:red;'>0点 No</span>
- 2点 Yes

<br>

### 5th Peer

#### Asignment

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r error=TRUE}
#### Peer-graded Assignment: The EM algorithm for Mixture Models
## Clear the environment and load required libraries
rm(list = ls())
set.seed(81196)

## Read data
#setwd("c:/Users/homeuk/Downloads/")
data = read.csv(file = 'data/fuses.csv', header = FALSE)
x    = data$V1
n    = length(x)
kk   = 2

# Plot the true density
par(mfrow = c(1, 1))
d <- density(x)
plot(d)

## Run the actual EM algorithm
## Initialize the parameters
w       = 0.01   # Assign small weight to exponential
lambda  = rexp(1, rate = 0.01) # random from exponential with very small rate
mu      = mean(log(x))  # Initial mean
tau     = sd(log(x))       # Initial tau
s       = 0
sw      = FALSE
QQ      = -Inf
QQ.out  = NULL
epsilon = 10^(-5)

## Checking convergence of the algorithm
  while(!sw){
    ## E step
    v = array(0, dim=c(400,2))
    v[,1] = log(w) + dexp(x, lambda, log=TRUE)    #Compute the log of the weights
    v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)  #Compute the log of the weights
    
    for(i in 1:n){
      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
    }
  
  ## M step
  # Weights
  w      = mean(v[,1])
  
  # Lambda
  lambda = 1/(sum(v[,1]*x)/sum(v[,1]))
  
  # mu
  mu     = sum(v[,2]*log(x))/sum(v[,2])
  
  # Tau
  tau    = sqrt(sum(v[,2]*(log(x)-mu)^2)/sum(v[,2]))
  
  ##Check convergence
  QQn    = 0
  for(i in 1:n){
    QQn  = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
      v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }
  
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
  
  #Plot current estimate over data
  layout(matrix(c(1, 2), 2, 1), widths = c(1, 1), heights = c(1.3, 3))
  par(mar = c(3.1, 4.1, 0.5, 0.5))
  plot(QQ.out[1:s], type = "l", xlim = c(1, max(10, s)), las = 1, ylab = "Q", lwd = 2)
  
  xx = seq(0.5, 10, length = 400)
  yy = w*dexp(xx, lambda) + (1-w)*dlnorm(xx, mu, tau)
  plot(xx, yy, type = "l", ylim = c(0, max(yy)), xlab = "x", ylab = "Final density")
  }
  
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

```{r error=TRUE}
c(w_hat,lambda_hat,mu_hat,tau_hat)
#[1] 0.09 3.05 0.78 0.38
```

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error=TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0点 No
- 1点 Some are, but not all
- <span style='color:Green;'>2点 Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error=TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error=TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- 1点 Some are correct, but not all of them
- <span style='color:Green;'>3点 Yes</span>

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error=TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- <span style='color:red;'>0点 No</span>
- 2点 Yes

<br>

### 6th Peer

#### Asignment

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r error=TRUE}
Fuses <- read.csv(file = "data/fuses.csv")

set.seed(81196)
x  = Fuses$X1.062163
w.hat = 1/2                         #Assign equal weight to each component to start with
lambda.hat = mean(x)   #Random cluster centers randomly spread over the support of the data
KK = 2
n = nrow(Fuses)
mu.hat = rnorm(1, mean(x), sd(x))
tau.hat = sd(x)

s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-5)

while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v[,1] = w.hat * dexp(x, lambda.hat)/
    (w.hat * dexp(x, lambda.hat) + (1 - w.hat) * dnorm(log(x),mu.hat,tau.hat)/x)
  v[,2] = (1 - w.hat) * dnorm(log(x),mu.hat,tau.hat)/x/
    (w.hat * dexp(x, lambda.hat) + (1 - w.hat) * dnorm(log(x),mu.hat,tau.hat)/x)
  
  ## M step
  # Weights
  w.hat = sum(v[,1])/(sum(v[,1]) + sum(v[,2]))
  lambda.hat = sum(v[,1])/sum(v[,1] * x)
  tau.hat = sqrt(sum(v[,2]*(log(x)-mu.hat)^2)/sum(v[,2]))
  mu.hat = sum(v[,2]*log(x))/(tau.hat^2*sum(v[,2]))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    QQn = QQn + v[i,1]*(log(w.hat) + log(lambda.hat) - lambda.hat * x[i]) +
      v[i,2]*(log(1-w.hat) - log(sqrt(2*pi)) -log(tau.hat*x[i]) - (log(x[i]) - mu.hat)^2/(2*tau.hat^2))
  }
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
}
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

```{r error=TRUE}
w.hat = 1.00
lambda.hat = 0.46
mu.hat = 2.15
tau.hat = 0.63

paste('w.hat =', w.hat)
paste('lambda.hat =', lambda.hat)
paste('mu.hat =', mu.hat)
paste('tau.hat =', tau.hat)
```

#### Marking

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error=TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0点 No
- 1点 Some are, but not all
- <span style='color:Green;'>2点 Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error=TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error=TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0点 No
- 1点 Some are correct, but not all of them
- <span style='color:Green;'>3点 Yes</span>

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error=TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0点 No
- <span style='color:Green;'>1点 Yes</span>

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- 0点 No
- <span style='color:Green;'>2点 Yes</span>

<br><br>

## ディスカッション

<br><br>

# Appendix

## Blooper

## Documenting File Creation 

It's useful to record some information about how your file was created.

- File creation date: 2021-05-12
- File latest updated date: `r today('Asia/Tokyo')`
- `r R.version.string`
- [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
- File version: 1.0.0
- Author Profile: [®γσ, Eng Lian Hu](https://github.com/scibrokes/owner)
- GitHub: [Source Code](https://github.com/englianhu/coursera-bayesian-statistics-mixture-models)
- Additional session information:

```{r info, warning=FALSE, error=TRUE, results='asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('magrittr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))
suppressMessages(require('knitr', quietly = TRUE))
suppressMessages(require('kableExtra', quietly = TRUE))

sys1 <- devtools::session_info()$platform %>%
  unlist %>%
  data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL

sys2 <- data.frame(Sys.info()) %>%
  dplyr::mutate(Category = rownames(.)) %>%
  .[2:1]
names(sys2)[2] <- c('Sys.info')
rownames(sys2) <- NULL

if (nrow(sys1) == 9 & nrow(sys2) == 8) {
  sys2 %<>% rbind(., data.frame(
  Category = 'Current time', 
  Sys.info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
} else {
  sys1 %<>% rbind(., data.frame(
  Category = 'Current time', 
  session_info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
}

sys <- cbind(sys1, sys2) %>%
  kbl(caption = 'Additional session information:') %>%
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>%
  row_spec(0, background = 'DimGrey', color = 'yellow') %>%
  column_spec(1, background = 'CornflowerBlue', color = 'red') %>%
  column_spec(2, background = 'grey', color = 'black') %>%
  column_spec(3, background = 'CornflowerBlue', color = 'blue') %>%
  column_spec(4, background = 'grey', color = 'white') %>%
  row_spec(9, bold = T, color = 'yellow', background = '#D7261E')

rm(sys1, sys2)
sys
```

## Reference

- [Bayesian Statistics: Mixture Models (Assessment 2 Codes)](https://rpubs.com/englianhu/767588)
- [Global CSS settings, fundamental HTML elements styled and enhanced with extensible classes, and an advanced grid system](https://getbootstrap.com/docs/3.3/css/#helper-classes-backgrounds)
- [Chunk option class.output is not working on Error Message](https://stackoverflow.com/a/55006240/3806250)
- [Width of R code chunk output in RMarkdown files knitr-ed to html](https://stackoverflow.com/a/36846864/3806250)
- [CodePen Home MathJax scale to fit container](https://codepen.io/mathjax/pen/qEdqPg)
- [align, aligned and R Markdown](https://tex.stackexchange.com/questions/284538/align-aligned-and-r-markdown)
- [**FR***: MathJax scale to fit container #2135](https://github.com/rstudio/rmarkdown/issues/2135)
- [rmarkdown-book/03-documents.Rmd : MathJax equations](https://github.com/rstudio/rmarkdown-book/blob/master/03-documents.Rmd)
- [Plot in R with `echarts4r`](https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html)
- [`echarts` : Theme Builder](https://echarts.apache.org/en/theme-builder.html)
- [Sample Code for EM Example 1](https://github.com/englianhu/Coursera-Bayesian-Statistics-Mixture-Models/blob/main/Sample-Code-for%20EM-EX1.R)

<br>

---

<br>
