---
title: "<img src='figure/coursera.jpg' width='37'> <img src='figure/ucsc.png' width='240'>"
subtitle: "<span style='color:white; background-color:#4E79A7;'>Bayesian Statistics: Mixture Models</span> (Assessment Week2 A Codes)"
author: "[Â®Î³Ïƒ, Lian Hu](https://englianhu.github.io/) <img src='figure/quantitative trader 1.jpg' width='12'> <img src='figure/ENG.jpg' width='24'> Â®"
date: "`r lubridate::today('Asia/Tokyo')`"
output:
  html_document: 
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    css: CSSBackgrounds.css
---

<br>
<span style='color:green'>**Theme Song**</span>
<br>

<audio src="music/California-Dreaming-Chorus.mp3" controls></audio>
<br>

------

<br>

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

*Source : [Width of R code chunk output in RMarkdown files knitr-ed to html](https://stackoverflow.com/a/36846864/3806250)*

```{css class.source = 'bg-success', class.output = 'bg-primary'}
/* https://stackoverflow.com/a/66029010/3806250 */
h1 { color: #002C54; }
h2 { color: #2F496E; }
h3 { color: #375E97; }

/* https://bookdown.org/yihui/rmarkdown-cookbook/chunk-styling.html */
.gradient1 {
  background: linear-gradient(155deg, #F9BA32 0%, #FFEB94 100%);
}

.gradient2 {
  color: #FFD64D;
  background: linear-gradient(155deg, #002C54 0%, #4CB5F5 100%);
  /*background-image: linear-gradient(to right,golden,golden, yellow, yellow);*/
  /*-webkit-background-clip: text;*/
  /*display: inline-block;*/
  /*padding: 14px;*/
  /*-webkit-text-fill-color: transparent;*/
  /*font-family: 'Stay Out Regular';*/
}

.shine {
	background: #FFD64D -webkit-gradient(linear, left top, right top, from(#222), to(#222), color-stop(0.5, yellow)) 0 0 no-repeat;
	-webkit-background-size: 150px;
	color: $text-color;
	-webkit-background-clip: text;
	-webkit-animation-name: shine;
	-webkit-animation-duration: $duration;
	-webkit-animation-iteration-count: infinite;
	text-shadow: 0 0px 0px golden-rod;
}
```

```{r global_options, class.source = 'bg-success', class.output = 'bg-primary'}
## https://stackoverflow.com/a/36846793/3806250
options(width = 999)
knitr::opts_chunk$set(class.source = 'gradient1', class.output = 'gradient2', class.error = 'bg-danger')
```

<br><br>

# å—è¬›ç”Ÿã«ã‚ˆã‚‹ãƒ†ã‚¹ãƒˆï¼šThe EM algorithm for Mixture Models

5æœˆ17æ—¥ 15:59 JST ã¾ã§ã«æå‡º

**èª²é¡Œã‚’ã™ãã«æå‡ºã—ã¦ãã ã•ã„**

èª²é¡Œã®æå‡ºæœŸé™ã¯ã€5æœˆ17æ—¥ 15:59 JSTã§ã™ãŒã€å¯èƒ½ã§ã‚ã‚Œã°1æ—¥ã‹2æ—¥æ—©ãæå‡ºã—ã¦ãã ã•ã„ã€‚

æ—©ã„æ®µéšã§æå‡ºã™ã‚‹ã¨ã€ä»–ã®å—è¬›ç”Ÿã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ™‚é–“å†…ã«å¾—ã‚‹å¯èƒ½æ€§ãŒé«˜ããªã‚Šã¾ã™ã€‚

<br><br>

## èª¬æ˜

Data on the lifetime (in years) of fuses produced by the ACME Corporation is available in the file `fuses.csv`:

In order to characterize the distribution of the lifetimes, it seems reasonable to fit to the data a two-component mixture of the form:

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

The first component, which corresponds to an exponential distribution with rate $\lambda$, is used to model low-quality components with a very short lifetime.  The second component, which corresponds to a log-Gaussian distribution, is used to model normal, properly-functioning components.

You are asked to modify the implementation of the EM algorithm contained in the Reading "Sample code for EM example 1" so that you can fit this two-component mixture distributions instead. You then should run your algorithm with the data contained in `fuses.csv` and report the values of the estimates for $w$, $\lambda$, $\mu$ and $\tau$ that you obtained, rounded to two decimal places.

<br><br>

### Review criteria

The code you generate should follow the same structure as "Sample code for EM example 1". Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect the fact that (1) you provided a reasonable initial point for you algorithm, (2) the observation-specific weights $v_{i,k}$ are computed correctly (E step), (3) the formulas for the maximum of the QQ functions are correct (M step), (4) the converge check is correct, and (5) the numerical values that you obtain are correct.

<br><br>

## è‡ªåˆ†ã®æå‡ºç‰©

## ãƒ”ã‚¢ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³

### ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯

#### Setup

```{r setup, warning = FALSE, message = FALSE}
if(!suppressPackageStartupMessages(require('BBmisc'))) {
  install.packages('BBmisc', dependencies = TRUE, INSTALL_opts = '--no-lock')
}
suppressPackageStartupMessages(require('BBmisc'))
# suppressPackageStartupMessages(require('rmsfuns'))

pkgs <- c('devtools', 'knitr', 'kableExtra', 'tidyr', 
          'readr', 'lubridate', 'data.table', 'reprex', 
          'timetk', 'plyr', 'dplyr', 'stringr', 'magrittr', 
          'tdplyr', 'tidyverse', 'formattable', 
          'echarts4r', 'paletteer')

suppressAll(lib(pkgs))
# load_pkg(pkgs)

## Set the timezone but not change the datetime
Sys.setenv(TZ = 'Asia/Tokyo')
## options(knitr.table.format = 'html') will set all kableExtra tables to be 'html', otherwise need to set the parameter on every single table.
options(warn = -1, knitr.table.format = 'html')#, digits.secs = 6)

## https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-abnor-in-rmd-but-not-in-r-script
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

rm(pkgs)
```

```{r error = TRUE}
dat <- fread('data/fuses.csv') %>% as.matrix
head(dat)
```

<span style='color:green;'>*Source : `r paste(dim(dat), collapse = ' x ')`*</span>

#### Assignment

```{r}
#read the data from the file
dat = read.csv(file = "data/fuses.csv", header = FALSE)
fuses = dat$V1
logfuses = log(fuses)

## Run the actual EM algorithm
## Initialize the parameters

KK         = 2                               # number of components
n = length(fuses)                         # number of samples
v = array(0, dim=c(n,KK))
v[,1] = 0.5                    #Assign half weight to first component
v[,2] = 1-v[,1]                #Assign all of the remaining weights to the second component
mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
lambda = 1.0/mean1            #parameter for the first component
mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
tau = sqrt(tausquared)
w = mean(v[,1])
print(paste(lambda, mu, tausquared, tau, w))


s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-5)

##Checking convergence of the algorithm
while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v[,1] = log(w) + dexp(fuses, lambda,log=TRUE)    #Compute the log of the weights
  v[,2] = log(1-w) + dnorm(logfuses, mu, tau, log=TRUE)  #Compute the log of the weights
  for(i in 1:n){
    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step
  # Weights
  w = mean(v[,1])
  ## parameters
  mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
  lambda = 1.0/mean1            #parameter for the first component
  mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
  tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
  tau = sqrt(tausquared)
  w = mean(v[,1])
  print(paste(s, lambda, mu, tausquared, tau, w))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    QQn = QQn + v[i,1]*(log(w) + dexp(fuses[i], lambda, log=TRUE)) +
      v[i,2]*(log(1-w) + dnorm(logfuses[i], mu, tau, log=TRUE))
  }
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
}

#Plot final estimate over data
layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))
par(mar=c(3.1,4.1,0.5,0.5))

#plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

## https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html
## https://echarts.apache.org/en/theme-builder.html
Qplot <- tibble(Index = 1:length(QQ.out[1:s]), Value = QQ.out[1:s])
Qplot %>% 
  e_charts(x = Index) %>% 
  e_line(Value) %>% 
  e_datazoom(
    type = "slider", 
    toolbox = FALSE,
    bottom = -5
  ) %>% 
  e_tooltip() %>% 
  e_title("Exopential Regression") %>% 
  e_x_axis(Index, axisPointer = list(show = TRUE)) %>% 
  e_legend(
    orient = 'vertical', 
    type = c('scroll'), 
    right = 80)
```

<br><br>

### ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯

**Are the initial values appropriate?**

The starting values of four parameters, $w$, $\lambda$, $\mu$ and $\tau$, need to be specified, and the context of the problem provides some useful clues.

Because the lognormal component corresponds to the "normal" components, and we expect the majority of the observations to be in this class, it makes sense to bias the weights so that $w \le 1/2$.  For example, we could use $w = 0.1$ (but other values that satisfy $w \le 1/2$ would be reasonable too).

For the same reason, a reasonable starting values for $\mu$ and $\tau$ correspond to their maximum likelihood estimators under the simpler log-Gaussian model. Since a random variable follows a log-Gaussian distribution if and only if its logarithm follows a Gaussian distribution, we can use $\mu = mean(log(x))$ and $\tau = sd(log(x))$ as our starting values.

Finally, because the defective components should have shorter lifespan than normal components, it makes sense to take $1/\lambda1$ (which is the mean of the first component) to be a small fraction of the overall mean of the data (we use $5%$ of the overall mean, but other similar values would all be reasonable).

In summary, you should expect an initialization such as this one:

```{r error = TRUE}
w      = 0.1
mu     = mean(log(x))
tau    = sd(log(x))
lambda = 20/mean(x)
```

- 0ç‚¹ No

- 1ç‚¹ Some are, but not all

- <span style='color:Green;'>2ç‚¹ Yes</span>

**Are the observation-specific weights $v_{i,k}$ computed correctly (E step)?  **

In this case it is easy to see that $v_{i,1}^{(t+1)} \propto w^{(t)} \lambda^{(t)} \exp\left\{ - \lambda^{(t)} x_i \right\}$ and $v_{i,2}^{(t+1)} \propto \left(1-w^{(t)}\right) \frac{1}{\sqrt{2\pi} \tau^{(t)} x_i} \exp\left\{ - \frac{1}{2 } \left( \frac{\log(x_i) - \mu^{(t)}}{\tau^{(t)}} \right)^2\right\}$.

Hence, the code for the E step could look something like

```{r error = TRUE}
## E step
v = array(0, dim=c(n,2))
v[,1] = log(w) + dexp(x, lambda, log=TRUE)    
v[,2] = log(1-w) + dlnorm(x, mu, tau, log=TRUE)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))
}
```

Remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R. For example, the calculation could be vectorized to increase efficiency.

- 0ç‚¹ No

- <span style='color:Green;'>1ç‚¹ Yes</span>

**Are the formulas for the maximum of the QQ functions correct (M step)?**

In this case

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Computing the derivatives and setting them to zero yields

$\hat{w}^{(t+1)} = \frac{1}{n}\sum_{i=1}^{n} v_{i,1}$

$\hat{\lambda}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,1}}{\sum_{i=1}^{n} v_{i,1} x_{i}}$

$\hat{\mu}^{(t+1)} = \frac{\sum_{i=1}^{n} v_{i,2} \log(x_i)}{\sum_{i=1}^{n} v_{i,2}}$

$\hat{\tau}^{(t+1)} = \sqrt{\frac{\sum_{i=1}^{n} v_{i,2}\left( \log x_i - \hat{\mu}^{(t+1)} \right)^2}{\sum_{i=1}^{n} v_{i,2}}}$ 

Hence, the code for this section might look something like:

```{r error = TRUE}
## M step
w      = mean(v[,1])
lambda = sum(v[,1])/sum(v[,1]*x)
mu     = sum(v[,2]*log(x))/sum(v[,2])
tau    = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))
```

However, remember to be open-minded when reviewing this code, as there are many ways to accomplish the same thing in R.

- 0ç‚¹ No

- 1ç‚¹ Some are correct, but not all of them

- <span style='color:Green;'>3ç‚¹ Yes</span>

**Is the converge check correct?**

As noted before, 

\begin{align}
Q\left(w, \lambda, \mu, \tau \mid \hat{w}^{(t)}, \hat{\lambda}^{(t)}, \hat{\mu}^{(t)} , \hat{\tau}^{(t)}\right) = \sum_{i=1}^{n} v_{i,1} \left[ \log w + \log \lambda - \lambda x_i \right] + \\
\quad\quad\quad v_{i,2}\left[ \log(1-w) - \frac{1}{2} \log(2 \pi) - \log \tau - \log x_i - \frac{1}{2\tau^2} \left( \log x_i - \mu \right) \right]
\end{align}

Hence, the convergence check might look something like

```{r error = TRUE}
QQn = 0
for(i in 1:n){
  QQn = QQn + v[i,1]*(log(w) + dexp(x[i], lambda, log=TRUE)) + 
    v[i,2]*(log(1-w) + dlnorm(x[i], mu, tau, log=TRUE))
  }

if(abs(QQn-QQ)/abs(QQn)<epsilon){
  sw=TRUE
  }
QQ = QQn
```

- 0ç‚¹ No

- <span style='color:Green;'>1ç‚¹ Yes</span>

### ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $\hat{w}=0.09$, $\hat{\lambda} = 3.05$, $\hat{\mu} = 0.78$ and $\hat{\tau}=0.38$.

- 0ç‚¹ No

- <span style='color:Green;'>2ç‚¹ Yes</span>

<br><br>

## ãƒ¬ãƒ“ãƒ¥ãƒ¼

### 1st Peer

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r}
dat = read.csv(file = "data/fuses.csv", header = FALSE)
x <- dat$V1
w     = 1/2        #Assign equal weight to each component to start with
sigma = sd(log(x)) # take sigma to be tau
mu <- c(.02, 2)
KK <- 2
QQ = -Inf
epsilon <- 10^(-8)

## E step
v = array(0, dim=c(n,KK))
v[,1] = log(w) + dexp(x, mu[1], log=T)
v[,2] = log(1-w) + dlnorm(x, mu[2], sigma, log=T)
for(i in 1:n){
  v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
}

## M step
# Weights
w = mean(v[,1])
mu[1] = sum(v[,1]) / sum(v[,1] * x)

mu[2] = sum(v[,2] * log(x)) / sum(v[,2])


# Standard deviations
sigma = sqrt(sum(v[,2]*(log(x) - mu)^2)/sum(v[,2]))

##Check convergence
QQn = 0
for(i in 1:n){
  comp1 = log(w) + dexp(x[i], mu[1], log=T);
  comp2 = log(1-w) + dlnorm(x[i], mu[2], sigma, log=T)
  QQn = QQn + v[i,1]*(comp1) + v[i,2]*(comp2)
}
#print(QQ)
if(abs(QQn-QQ)/abs(QQn)<epsilon){sw=TRUE}
QQ = QQn

QQ
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

$w = 0.09, lambda = 3.05, mu = 0.79, tau = 0.38$

#### ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $w = 0.09$, $lambda = 3.05$, $mu = 0.79$, $tau = 0.38$.

- 0ç‚¹ No

- <span style='color:Green;'>2ç‚¹ Yes</span>

<br><br>

### 2nd Peer

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**

Provide the EM algorithm to fit the mixture model

\begin{align}
f(x) = w \lambda \exp\left\{ -\lambda x \right\} + (1-w)  \frac{1}{\sqrt{2\pi} \tau x} \exp\left\{ - \frac{1}{2 \tau^2} \left( \log(x) - \mu \right)^2\right\} \\ \quad\quad\quad x>0.
\end{align}

```{r}
#read the data from the file
dat = read.csv(file = "data/fuses.csv", header = FALSE)
fuses = dat$V1
logfuses = log(fuses)

## Run the actual EM algorithm
## Initialize the parameters

KK         = 2                               # number of components
n = length(fuses)                         # number of samples
v = array(0, dim=c(n,KK))
v[,1] = 0.5                    #Assign half weight to first component
v[,2] = 1-v[,1]                #Assign all of the remaining weights to the second component
mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
lambda = 1.0/mean1            #parameter for the first component
mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
tau = sqrt(tausquared)
w = mean(v[,1])
print(paste(lambda, mu, tausquared, tau, w))


s  = 0
sw = FALSE
QQ = -Inf
QQ.out = NULL
epsilon = 10^(-5)

##Checking convergence of the algorithm
while(!sw){
  ## E step
  v = array(0, dim=c(n,KK))
  v[,1] = log(w) + dexp(fuses, lambda,log=TRUE)    #Compute the log of the weights
  v[,2] = log(1-w) + dnorm(logfuses, mu, tau, log=TRUE)  #Compute the log of the weights
  for(i in 1:n){
    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step
  # Weights
  w = mean(v[,1])
  ## parameters
  mean1 = sum(v[,1]*fuses)/sum(v[, 1]) #mean of the first component
  lambda = 1.0/mean1            #parameter for the first component
  mu = sum(v[,2]*logfuses)/sum(v[,2])    #parameter (mean) of the second component
  tausquared = sum(v[,2]*((logfuses-mu)**2))/sum(v[,2]) #parameter (variance) for the second component
  tau = sqrt(tausquared)
  w = mean(v[,1])
  print(paste(s, lambda, mu, tausquared, tau, w))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    QQn = QQn + v[i,1]*(log(w) + dexp(fuses[i], lambda, log=TRUE)) +
      v[i,2]*(log(1-w) + dnorm(logfuses[i], mu, tau, log=TRUE))
  }
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
}

#Plot final estimate over data
layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))
par(mar=c(3.1,4.1,0.5,0.5))
plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)
```

Provide you maximum likelihood estimates $\hat{\omega}$, $\hat{\lambda}$, $\hat{\mu}$ and $\hat{\tau}$, **rounded** to two decimal places.

$w = 0.09, lambda = 3.05, mu = 0.79, tau = 0.38$

#### ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯

**Are the estimates of the parameters generated by the algorithm correct?**

The point estimates, rounded to two decimal places, are $w = 0.09$, $lambda = 3.05$, $mu = 0.79$, $tau = 0.38$.

- 0ç‚¹ No

- <span style='color:Green;'>2ç‚¹ Yes</span>

<br><br>

## ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³

<br><br>

# Appendix

## Blooper

## Documenting File Creation 

It's useful to record some information about how your file was created.

- File creation date: 2021-05-12
- File latest updated date: `r today('Asia/Tokyo')`
- `r R.version.string`
- [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
- File version: 1.0.0
- Author Profile: [Â®Î³Ïƒ, Eng Lian Hu](https://github.com/scibrokes/owner)
- GitHub: [Source Code](https://github.com/englianhu/coursera-bayesian-statistics-mixture-models)
- Additional session information:

```{r info, warning = FALSE, results = 'asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('magrittr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))
suppressMessages(require('knitr', quietly = TRUE))
suppressMessages(require('kableExtra', quietly = TRUE))

sys1 <- devtools::session_info()$platform %>% 
  unlist %>% data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL

sys2 <- data.frame(Sys.info()) %>% 
  dplyr::mutate(Category = rownames(.)) %>% .[2:1]
names(sys2)[2] <- c('Sys.info')
rownames(sys2) <- NULL

if (nrow(sys1) == 9 & nrow(sys2) == 8) {
  sys2 %<>% rbind(., data.frame(
  Category = 'Current time', 
  Sys.info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JSTğŸ—¾')))
} else {
  sys1 %<>% rbind(., data.frame(
  Category = 'Current time', 
  session_info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JSTğŸ—¾')))
}

sys <- cbind(sys1, sys2) %>% 
  kbl(caption = 'Additional session information:') %>% 
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>% 
  row_spec(0, background = 'DimGrey', color = 'yellow') %>% 
  column_spec(1, background = 'CornflowerBlue', color = 'red') %>% 
  column_spec(2, background = 'grey', color = 'black') %>% 
  column_spec(3, background = 'CornflowerBlue', color = 'blue') %>% 
  column_spec(4, background = 'grey', color = 'white') %>% 
  row_spec(9, bold = T, color = 'yellow', background = '#D7261E')

rm(sys1, sys2)
sys
```

## Reference

- [Bayesian Statistics: Mixture Models (Assessment 2 Codes)](https://rpubs.com/englianhu/767588)
- [Global CSS settings, fundamental HTML elements styled and enhanced with extensible classes, and an advanced grid system](https://getbootstrap.com/docs/3.3/css/#helper-classes-backgrounds)
- [Chunk option class.output is not working on Error Message](https://stackoverflow.com/a/55006240/3806250)
- [Width of R code chunk output in RMarkdown files knitr-ed to html](https://stackoverflow.com/a/36846864/3806250)
- [CodePen Home MathJax scale to fit container](https://codepen.io/mathjax/pen/qEdqPg)
- [align, aligned and R Markdown](https://tex.stackexchange.com/questions/284538/align-aligned-and-r-markdown)
- [**FR***: MathJax scale to fit container #2135](https://github.com/rstudio/rmarkdown/issues/2135)
- [rmarkdown-book/03-documents.Rmd : MathJax equations](https://github.com/rstudio/rmarkdown-book/blob/master/03-documents.Rmd)
- [Plot in R with `echarts4r`](https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html)
- [`echarts` : Theme Builder](https://echarts.apache.org/en/theme-builder.html)

<br>

---

<br>
