---
title: "<img src='figure/coursera.jpg' width='37'> <img src='figure/ucsc.png' width='240'>"
subtitle: "<span style='color:white; background-color:#4E79A7;'>Bayesian Statistics: Mixture Models</span> (Assessment Week4B with Codes)"
author: "[®γσ, Lian Hu](https://englianhu.github.io/) <img src='figure/quantitative trader 1.jpg' width='12'> <img src='figure/ENG.jpg' width='24'> ®"
date: "`r lubridate::today('Asia/Tokyo')`"
output:
  html_document: 
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    number_sections: yes
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    css: CSSBackgrounds.css
---

<br>
<span style='color:green'>**Theme Song**</span>
<br>

<audio src="music/California-Dreaming-Chorus.mp3" controls></audio>
<br>

------

# Setting

## SCSS Setup

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
.table-hover > tbody > tr:hover { 
  background-color: #8D918D;
}
</style>

```{r class.source='bg-success', class.output='bg-primary', message = FALSE, warning = FALSE}
# install.packages("remotes")
library('BBmisc', 'rmsfuns')
#remotes::install_github("rstudio/sass")
lib('sass')
```

```{scss class.source='bg-success', class.output='bg-primary'}
/* https://stackoverflow.com/a/66029010/3806250 */
h1 { color: #002C54; }
h2 { color: #2F496E; }
h3 { color: #375E97; }
h4 { color: #556DAC; }
h5 { color: #92AAC7; }

/* ----------------------------------------------------------------- */
/* https://gist.github.com/himynameisdave/c7a7ed14500d29e58149#file-broken-gradient-animation-less */
.hover01 {
  /* color: #FFD64D; */
  background: linear-gradient(155deg, #EDAE01 0%, #FFEB94 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #EDAE01 20%, #FFEB94 80%);
    }
  }

.hover02 {
  color: #FFD64D;
  background: linear-gradient(155deg, #002C54 0%, #4CB5F5 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #002C54 20%, #4CB5F5 80%);
    }
  }

.hover03 {
  color: #FFD64D;
  background: linear-gradient(155deg, #A10115 0%, #FF3C5C 100%);
  transition: all 0.45s;
  &:hover{
    background: linear-gradient(155deg, #A10115 20%, #FF3C5C 80%);
    }
  }
```

```{r global_options, class.source='hover01', class.output='hover02'}
## https://stackoverflow.com/a/36846793/3806250
options(width = 999)
knitr::opts_chunk$set(class.source = 'hover01', class.output = 'hover02', class.error = 'hover03')
```

<br><br>

## Setup

```{r warning=FALSE, message=FALSE}
if(!suppressPackageStartupMessages(require('BBmisc'))) {
  install.packages('BBmisc', dependencies = TRUE, INSTALL_opts = '--no-lock')
}
suppressPackageStartupMessages(require('BBmisc'))
# suppressPackageStartupMessages(require('rmsfuns'))

pkgs <- c('devtools', 'knitr', 'kableExtra', 'tidyr', 
          'readr', 'lubridate', 'data.table', 'reprex', 
          'timetk', 'plyr', 'dplyr', 'stringr', 'magrittr', 
          'tdplyr', 'tidyverse', 'formattable', 
          'echarts4r', 'paletteer')

suppressAll(lib(pkgs))
# load_pkg(pkgs)

## Set the timezone but not change the datetime
Sys.setenv(TZ = 'Asia/Tokyo')
## options(knitr.table.format = 'html') will set all kableExtra tables to be 'html', otherwise need to set the parameter on every single table.
options(warn = -1, knitr.table.format = 'html')#, digits.secs = 6)

## https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-abnor-in-rmd-but-not-in-r-script
knitr::opts_chunk$set(message = FALSE, warning = FALSE)#, 
                      #cache = TRUE, cache.lazy = FALSE)

rm(pkgs)
```

<br><br>

# 受講生によるテスト：Classification

**課題をすぐに提出してください**

課題の提出期限は、6月14日 15:59 JSTですが、可能であれば1日か2日早く提出してください。 早い段階で提出すると、他の受講生のレビューを時間内に得る可能性が高くなります。

## 説明

The data set banknote contains six measurements (length of bill, width of left edge, width of right edge, bottom margin width, top margin width, and length of diagonal, all in mm) made on 100 genuine and 100 counterfeit old-Swiss 1000-franc bank notes:

[`banknoteclassification.Rdata`](https://github.com/englianhu/Coursera-Bayesian-Statistics-Mixture-Models/blob/main/data/banknoteclassification.RData)

To load the dataset in R, use the command `load("banknoteclassification.Rdata")`, but first make sure that your working directory is set to the directory containing the file.  You should see four objects:

- `banknote.training` contains the characteristics for 30 notes (15 genuine and 15 counterfeit) in the training set.
- `banknote.training.labels` contains the labels ("genuine" or "counterfeit") for the 30 notes in the training set
- `banknote.test` contains the characteristics for 170 notes (85 genuine and 85 counterfeit) in the test set.
- `banknote.test.labels` contains the labels ("genuine" or "counterfeit") for the 170 notes in the test set.  These are provided only for validation purposes.
- You are asked to modify the MCMC algorithm in "Sample code for MCMC example 2" to create an algorithm for semi-supervised classification that is the Bayesian equivalent of that provided under "Sample EM algorithm for classification problems" and apply it to classify the observations contained in the test set.  You are then asked to compare your results against those generated by the `qda` function in R.  

As your priors, use distributions in the same families as in "Sample code for MCMC example 2".  In particular, use a uniform distribution for the weights, multivariate normal distributions for the means of the components, and inverse Wishart priors for the variance-covariance matrices of the components.  The parameters of the priors should be set using the same empirical Bayes approach used in that example.

<br><br>

### Review criteria

Reviewers will check that the code has been properly adapted, and whether the classification results you provide are correct.

<br><br>

## 自分の提出物

### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```
·
Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```


### Marking


<br><br>

## ピアレビュー

### 1st Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}
n = dim(banknote.training)[1]  # Size of the training set
m = dim(banknote.test)[1]      # Size of the test set
x = rbind(banknote.training, banknote.test)   # Create dataset of observations, first n belong to the training set, and the rest belong to the test set
p       = dim(banknote.training)[2]              # Number of features
KK      = 2

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK

cc = rep(NA, n+m)
cc[1:n] = as.numeric(banknote.training.labels)


# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/2

# Number of iteration of the sampler
rrr = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, (n+m)))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in 1:(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      #Compute the log of the weights
      v[k] = log(w[k]) + mvtnorm::dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    if (i>n) cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:(n+m)){
    logpost[s] = logpost[s] + log(w[cc[i]]) + mvtnorm::dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + mvtnorm::dmvnorm(mu[k,], dd, DD, log=TRUE)
    logpost[s] = logpost[s] + log(diwish(Sigma[k,,], nu, SS))
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}
```

What is the classification error for the test set?

```{r error=TRUE}
0
```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}
3
```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 2nd Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}
n = dim(banknote.training)[1]  # Size of the training set
m = dim(banknote.test)[1]      # Size of the test set
x = rbind(banknote.training, banknote.test)   # Create dataset of observations, first n belong to the training set, and the rest belong to the test set
p       = dim(banknote.training)[2]              # Number of features
KK      = 2

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK

cc = rep(NA, n+m)
cc[1:n] = as.numeric(banknote.training.labels)


# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/2

# Number of iteration of the sampler
rrr = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, (n+m)))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in 1:(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      #Compute the log of the weights
      v[k] = log(w[k]) + mvtnorm::dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    if (i>n) cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:(n+m)){
    logpost[s] = logpost[s] + log(w[cc[i]]) + mvtnorm::dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + mvtnorm::dmvnorm(mu[k,], dd, DD, log=TRUE)
    logpost[s] = logpost[s] + log(diwish(Sigma[k,,], nu, SS))
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}
```

What is the classification error for the test set?

```{r error=TRUE}
0
```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}
3
```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 3rd Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 4th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 5th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 6th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 7th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 8th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 9th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br>

### 10th Peer

#### Assignment

Provide an MCMC algorithm to fit a semisupervised Bayesian quadratic discriminant model to  the banknote data.

```{r error=TRUE}

```

What is the classification error for the test set?

```{r error=TRUE}

```

Is the R function `qda` (which implements classical quadratic discriminant analysis) is used to classify the observations in the test set, what is the classification error?

```{r error=TRUE}

```

#### Marking

**Is the setup of the algorithm correct?**

Recall that the semisupervised version of the algorithm uses all observations but treats the labels for he training set as known.  There are many ways to set this up, but one that requires minimal changes to the algorithm defines the $x$ object in the code as the combination of the observations in the training and test sets,  then defines $n$, $m$, $K$ and $p$ based on the dimensions of the original objects, and initialize the component indicators of the observations in the training set to their true (known) values:

```{r error=TRUE}
## All observations used for calculation
x   = rbind(banknote.training,banknote.test)

## Size of the training and test sets
n   = dim(banknote.training)[1]
m   = dim(unique(banknote.test))[1]

## Number of components and dimensionality of the components
KK  = length(unique(banknote.training.labels))
p   = dim(banknote.training)[2]

## Starting value for the indicators
## Note that for the training set we use the true values, while for the
## test set the values are initialized randomly
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))


```

However, be open minded in how you evaluate this item. The most important thing is that the setup is consistent with the rest of the implementation, and that it enables the use of both the training and test sets for the estimation of the means and variance of the components.

- 0点 No
- 1点 Yes

**Is the sampler for $c$ correct?**

The full conditional for the indicators is identical to the one in the original code. The only difference is that the labels for the training set are considered known, and therefore not sampled. In practice, this means a simple change to the values over which the $for$ loop iterates:

```{r error=TRUE}
# Sample the indicators
for(i in (n+1):(n+m)){
  v = rep(0,KK)
  for(k in 1:KK){
    v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
  }
  v = exp(v - max(v))/sum(exp(v - max(v)))
  cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
}
```

Because the code never changes the values of the first $n$ entries of $cc$, it is important that the setup of the algorithm (see previous prompt in the rubric) initializes them to the true known values.

- 0点 No
- 1点 Yes

**Is the sampler for the $\mu_k$s correct?**

Again, the sampler is identical to the one used before.  The only difficulty here is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals. In the case of the means that is easy (no change is required, as the sample size is not used explicitly).

```{r error=TRUE}
# Sample the means
DD.st = matrix(0, nrow=p, ncol=p)
for(k in 1:KK){
  mk    = sum(cc==k)
  xsumk = apply(x[cc==k,], 2, sum)
  DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
  dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
  mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
}
```

- 0点 No
- 1点 Yes

**Is the sampler for the $\sum_k$s correct?**

Again, the sampler is identical to the one used before. The only difficulty is that, because $n$ has a slightly different meaning here than in the original code (it is the size of the training set rather than the total sample size) you need to be careful to ensure the code uses all observations to compute the parameters of the full conditionals.  In this case, a change in the code **<u>is</u>** needed as the sample size is explicitly used in the code (see the upper limit of the for loop in line 3 below).

```{r error=TRUE}
# Sample the variances
xcensumk = array(0, dim=c(KK,p,p))
for(i in 1:(n+m)){  ## Need to loop over all (n+m) observations, not just the first n
  xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
}
for(k in 1:KK){
  Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
}
```

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

The classification is based on the probabilities that each observation is classified as "genuine" and "counterfeit". Those probabilities can be estimated using the frequencies for which each 
```{r error=TRUE}
probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

A sample of the full code for this problem is provided below:

```{r error=TRUE}
#### Semisupervised classification for the banknote dataset
rm(list=ls())
library(mvtnorm)
library(MCMCpack)

## Load data
load("banknoteclassification.Rdata")
x = rbind(banknote.training,banknote.test)

## Generate data from a mixture with 3 components
KK      = length(unique(banknote.training.labels))
p       = dim(banknote.training)[2]
n       = dim(banknote.training)[1]
m       = dim(unique(banknote.test))[1]

## Initialize the parameters
w          = rep(1,KK)/KK  #Assign equal weight to each component to start with
mu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data
Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same
Sigma[1,,] = var(x)/KK  
Sigma[2,,] = var(x)/KK
cc         = c(as.numeric(banknote.training.labels), sample(1:KK, m, replace=TRUE, prob=w))

# Priors
aa = rep(1, KK)
dd = apply(x,2,mean)
DD = 10*var(x)
nu = p+1
SS = var(x)/3

# Number of iteration of the sampler
rrr  = 11000
burn = 1000

# Storing the samples
cc.out    = array(0, dim=c(rrr, n+m))
w.out     = array(0, dim=c(rrr, KK))
mu.out    = array(0, dim=c(rrr, KK, p))
Sigma.out = array(0, dim=c(rrr, KK, p, p))
logpost   = rep(0, rrr)

for(s in 1:rrr){
  # Sample the indicators
  for(i in (n+1):(n+m)){
    v = rep(0,KK)
    for(k in 1:KK){
      v[k] = log(w[k]) + dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights
    }
    v = exp(v - max(v))/sum(exp(v - max(v)))
    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)
  }
  
  # Sample the weights
  w = as.vector(rdirichlet(1, aa + tabulate(cc)))
  
  # Sample the means
  DD.st = matrix(0, nrow=p, ncol=p)
  for(k in 1:KK){
    mk    = sum(cc==k)
    xsumk = apply(x[cc==k,], 2, sum)
    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))
    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)
    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))
  }
  
  # Sample the variances
  xcensumk = array(0, dim=c(KK,p,p))
  for(i in 1:(n+m)){
    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])
  }
  for(k in 1:KK){
    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])
  }
  
  # Store samples
  cc.out[s,]      = cc
  w.out[s,]       = w
  mu.out[s,,]     = mu
  Sigma.out[s,,,] = Sigma
  for(i in 1:n){
    logpost[s] = logpost[s] + log(w[cc[i]]) + dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)
  }
  logpost[s] = logpost[s] + ddirichlet(w, aa)
  for(k in 1:KK){
    logpost[s] = logpost[s] + dmvnorm(mu[k,], dd, DD)
    logpost[s] = logpost[s] + diwish(Sigma[k,,], nu, SS)
  }
  
  if(s/250==floor(s/250)){
    print(paste("s = ", s))
  }
}


probgenuine = rep(NA, m)
for(i in 1:m){
  probgenuine[i] = sum(cc.out[-seq(1,burn),n+i]==2)/(rrr-burn)
}
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set).

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

- 0点 No
- 1点 Yes

**Is the code used to classify observations in the test set correct?**

This is a simple call to the `qda` function

```{r error=TRUE}
banknote.qda = qda(x=banknote.training, grouping=banknote.training.labels)
predict(banknote.qda, banknote.test)$class
```

- 0点 No
- 1点 Yes

**Is the classification error for the "genuine" class generated by the algorithm correct?**

The value should be 0% (the algorithm perfectly classifies genuine banknotes in the test set). This is the same as the semisupervised Bayesian QDA algorithm you just implement

- 0点 No
- 1点 Yes

**Is the classification error for the "counterfeit" class generated by the algorithm correct?**

The value should also be 3.52% (3 errors out of 85 observations).  In this case, the function $qda$ underperforms compared with the semisupervised Bayesian QDA algorithm.

- 0点 No
- 1点 Yes

<br><br>

## ディスカッション

<br><br>

# Appendix

## Blooper

## Documenting File Creation 

It's useful to record some information about how your file was created.

- File creation date: 2021-05-24
- File latest updated date: `r today('Asia/Tokyo')`
- `r R.version.string`
- [**rmarkdown** package](https://github.com/rstudio/rmarkdown) version: `r packageVersion('rmarkdown')`
- File version: 1.0.0
- Author Profile: [®γσ, Eng Lian Hu](https://github.com/scibrokes/owner)
- GitHub: [Source Code](https://github.com/englianhu/coursera-bayesian-statistics-mixture-models)
- Additional session information:

```{r info, warning=FALSE, error=TRUE, results='asis'}
suppressMessages(require('dplyr', quietly = TRUE))
suppressMessages(require('magrittr', quietly = TRUE))
suppressMessages(require('formattable', quietly = TRUE))
suppressMessages(require('knitr', quietly = TRUE))
suppressMessages(require('kableExtra', quietly = TRUE))

sys1 <- devtools::session_info()$platform %>% 
  unlist %>% data.frame(Category = names(.), session_info = .)
rownames(sys1) <- NULL

sys2 <- data.frame(Sys.info()) %>% 
  dplyr::mutate(Category = rownames(.)) %>% .[2:1]
names(sys2)[2] <- c('Sys.info')
rownames(sys2) <- NULL

if (nrow(sys1) == 9 & nrow(sys2) == 8) {
  sys2 %<>% rbind(., data.frame(
  Category = 'Current time', 
  Sys.info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
} else {
  sys1 %<>% rbind(., data.frame(
  Category = 'Current time', 
  session_info = paste(as.character(lubridate::now('Asia/Tokyo')), 'JST🗾')))
}

sys <- cbind(sys1, sys2) %>% 
  kbl(caption = 'Additional session information:') %>% 
  kable_styling(bootstrap_options = c('striped', 'hover', 'condensed', 'responsive')) %>% 
  row_spec(0, background = 'DimGrey', color = 'yellow') %>% 
  column_spec(1, background = 'CornflowerBlue', color = 'red') %>% 
  column_spec(2, background = 'grey', color = 'black') %>% 
  column_spec(3, background = 'CornflowerBlue', color = 'blue') %>% 
  column_spec(4, background = 'grey', color = 'white') %>% 
  row_spec(9, bold = T, color = 'yellow', background = '#D7261E')

rm(sys1, sys2)
sys
```

## Reference

<br><br>
